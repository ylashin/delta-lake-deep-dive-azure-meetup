{"cells":[{"cell_type":"markdown","source":["# Fundamentals"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"041031b3-1ad2-4728-ac14-0a093d7faddc"},{"cell_type":"markdown","source":["## Protocol\r\n","\r\n","See [Delta Transaction Log Protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md#delta-transaction-log-protocol)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4aeacf14-c223-4654-91a2-6755c80e5969"},{"cell_type":"code","source":["protocol = {\r\n","    \"protocol\": {\r\n","        \"minReaderVersion\": 1,\r\n","        \"minWriterVersion\": 1\r\n","    }\r\n","}\r\n","print(protocol)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:22.457039Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:23.7222194Z","execution_finish_time":"2023-07-19T03:04:24.0773756Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"aa32ad01-ef14-4548-823c-cdda6d14432a"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'protocol': {'minReaderVersion': 1, 'minWriterVersion': 1}}\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f776533a-13ba-449f-b2d8-628070b070f4"},{"cell_type":"markdown","source":["## Tables need a schema"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bed822d5-e0ee-4882-aca9-545882146fab"},{"cell_type":"code","source":["from uuid import uuid4\r\n","from pprint import pprint\r\n","import time\r\n","import json\r\n","\r\n","schema = {\r\n","    \"type\":\"struct\",\r\n","    \"fields\":[\r\n","        {\"name\":\"id\", \"type\":\"long\", \"nullable\":False, \"metadata\":{}},\r\n","        {\"name\":\"name\", \"type\":\"string\", \"nullable\":False, \"metadata\":{}},\r\n","    ]\r\n","}\r\n","\r\n","metadata = {\r\n","    \"metaData\": {\r\n","        \"id\": str(uuid4()),\r\n","        \"name\": None,\r\n","        \"description\": None,\r\n","        \"format\": {\r\n","            \"provider\": \"parquet\", # Theoritically speaking other formats can be used but the community uses parquet everywhere.\r\n","            \"options\": {}\r\n","        },\r\n","        \"schemaString\": json.dumps(schema),\r\n","        \"partitionColumns\": [],\r\n","        \"createdTime\": int(time.time_ns() / 1000000),\r\n","        \"configuration\": {}\r\n","    }\r\n","}\r\n","pprint(metadata)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:22.5794231Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:24.6592729Z","execution_finish_time":"2023-07-19T03:04:25.0704588Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a3d0798b-6198-411c-9b56-6eef8b25a71f"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'metaData': {'configuration': {},\n              'createdTime': 1689735864575,\n              'description': None,\n              'format': {'options': {}, 'provider': 'parquet'},\n              'id': '8a4e6c15-1e57-40a6-b7bc-97c9c806fc31',\n              'name': None,\n              'partitionColumns': [],\n              'schemaString': '{\"type\": \"struct\", \"fields\": [{\"name\": \"id\", '\n                              '\"type\": \"long\", \"nullable\": false, \"metadata\": '\n                              '{}}, {\"name\": \"name\", \"type\": \"string\", '\n                              '\"nullable\": false, \"metadata\": {}}]}'}}\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a13538b3-922a-4118-b3af-5a849781c53f"},{"cell_type":"markdown","source":["## Helper function to write a panda in parquet"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a185cc09-52eb-4d1f-a609-1272d0a6f2ff"},{"cell_type":"code","source":["import pandas as pd\r\n","def write_parquet_file(data, path: str):\r\n","    df = pd.DataFrame(data)\r\n","    if not path.startswith(\"/lakehouse/default/\"):\r\n","        path = f\"/lakehouse/default/{path}\"\r\n","    df.to_parquet(path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:22.6849316Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:25.589874Z","execution_finish_time":"2023-07-19T03:04:25.9667727Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"56dbfb04-1948-4b43-bece-17a1a0d662a1"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"62a6e62b-a48e-4787-a0bf-b02d8860c431"},{"cell_type":"markdown","source":["## Create an empty table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"32bb1367-dccb-4ec3-ae3d-943167e00f8d"},{"cell_type":"code","source":["import uuid\r\n","\r\n","root_path = \"Files/delta-lake/1-fundamentals\"\r\n","\r\n","if mssparkutils.fs.exists(root_path):\r\n","    mssparkutils.fs.rm(root_path, recurse=True)\r\n","\r\n","table_path = f\"{root_path}/{uuid.uuid4()}\"\r\n","\r\n","protcol_str = json.dumps(protocol)\r\n","metadata_str = json.dumps(metadata)\r\n","\r\n","content = protcol_str + \"\\n\" + metadata_str\r\n","\r\n","mssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000000.json\", content)\r\n","spark.read.format(\"delta\").load(table_path).show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:22.7838725Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:26.5197323Z","execution_finish_time":"2023-07-19T03:04:30.5125163Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":4,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":1362,"rowCount":2,"jobId":10,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 8:\nimport uuid\n\nroot_path = \"Files/delta-lake/1-fundamentals\"\n\nif mssparkutils.fs.exists(root_path):\n    mssparkutils.fs.rm(root_path, recurse=True)\n\ntable_path = f\"{root_path}/{uuid.uuid4()}\"\n\nprotcol_str = json.dumps(protocol)\nmetadata_str = json.dumps(metadata)\n\ncontent = protcol_str + \"\n\" + metadata_str\n\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000000.json\", content)\nspark.read.format(\"delta\").load(table_path).show(): Filtering files for query","submissionTime":"2023-07-19T03:04:29.543GMT","completionTime":"2023-07-19T03:04:29.997GMT","stageIds":[16,17],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4308,"rowCount":50,"jobId":9,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nimport uuid\n\nroot_path = \"Files/delta-lake/1-fundamentals\"\n\nif mssparkutils.fs.exists(root_path):\n    mssparkutils.fs.rm(root_path, recurse=True)\n\ntable_path = f\"{root_path}/{uuid.uuid4()}\"\n\nprotcol_str = json.dumps(protocol)\nmetadata_str = json.dumps(metadata)\n\ncontent = protcol_str + \"\n\" + metadata_str\n\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000000.json\", content)\nspark.read.format(\"delta\").load(table_path).show(): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:04:29.122GMT","completionTime":"2023-07-19T03:04:29.163GMT","stageIds":[15,13,14],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4308,"dataRead":557,"rowCount":52,"jobId":8,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nimport uuid\n\nroot_path = \"Files/delta-lake/1-fundamentals\"\n\nif mssparkutils.fs.exists(root_path):\n    mssparkutils.fs.rm(root_path, recurse=True)\n\ntable_path = f\"{root_path}/{uuid.uuid4()}\"\n\nprotcol_str = json.dumps(protocol)\nmetadata_str = json.dumps(metadata)\n\ncontent = protcol_str + \"\n\" + metadata_str\n\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000000.json\", content)\nspark.read.format(\"delta\").load(table_path).show(): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:04:28.406GMT","completionTime":"2023-07-19T03:04:29.099GMT","stageIds":[12,11],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":557,"dataRead":499,"rowCount":4,"jobId":7,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nimport uuid\n\nroot_path = \"Files/delta-lake/1-fundamentals\"\n\nif mssparkutils.fs.exists(root_path):\n    mssparkutils.fs.rm(root_path, recurse=True)\n\ntable_path = f\"{root_path}/{uuid.uuid4()}\"\n\nprotcol_str = json.dumps(protocol)\nmetadata_str = json.dumps(metadata)\n\ncontent = protcol_str + \"\n\" + metadata_str\n\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000000.json\", content)\nspark.read.format(\"delta\").load(table_path).show(): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:04:27.614GMT","completionTime":"2023-07-19T03:04:28.141GMT","stageIds":[10],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"58c96c2f-5d63-4ac3-a647-1741f24839e7"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+----+\n| id|name|\n+---+----+\n+---+----+\n\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"bc59060d-7d5f-40f4-8157-f73672bada79"},{"cell_type":"markdown","source":["## Add a few records to table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6468195e-32f8-46d9-9e7e-70ecc465318b"},{"cell_type":"code","source":["write_parquet_file( \r\n","    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \r\n","    f\"{table_path}/file1.parquet\"\r\n",")\r\n","\r\n","add = {\r\n","    \"add\": {\r\n","        \"path\": \"file1.parquet\",\r\n","        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\r\n","        \"partitionValues\": {},\r\n","        \"modificationTime\": int(time.time_ns() / 1000000),\r\n","        \"dataChange\": True,\r\n","        \"tags\": None\r\n","    }\r\n","}\r\n","\r\n","add_content = json.dumps(add)\r\n","mssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\r\n","\r\n","display(spark.read.format(\"delta\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:22.9316361Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:31.0924835Z","execution_finish_time":"2023-07-19T03:04:35.0018477Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":3914,"rowCount":2,"jobId":15,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 9:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \n    f\"{table_path}/file1.parquet\"\n)\n\nadd = {\n    \"add\": {\n        \"path\": \"file1.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nadd_content = json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path))","submissionTime":"2023-07-19T03:04:34.318GMT","completionTime":"2023-07-19T03:04:34.448GMT","stageIds":[26],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1473,"rowCount":3,"jobId":14,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 9:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \n    f\"{table_path}/file1.parquet\"\n)\n\nadd = {\n    \"add\": {\n        \"path\": \"file1.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nadd_content = json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:33.962GMT","completionTime":"2023-07-19T03:04:34.231GMT","stageIds":[24,25],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4319,"rowCount":50,"jobId":13,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \n    f\"{table_path}/file1.parquet\"\n)\n\nadd = {\n    \"add\": {\n        \"path\": \"file1.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nadd_content = json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:33.657GMT","completionTime":"2023-07-19T03:04:33.723GMT","stageIds":[21,22,23],"jobGroup":"9","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4319,"dataRead":901,"rowCount":53,"jobId":12,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \n    f\"{table_path}/file1.parquet\"\n)\n\nadd = {\n    \"add\": {\n        \"path\": \"file1.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nadd_content = json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:32.973GMT","completionTime":"2023-07-19T03:04:33.628GMT","stageIds":[19,20],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":901,"dataRead":639,"rowCount":6,"jobId":11,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second\"]}, \n    f\"{table_path}/file1.parquet\"\n)\n\nadd = {\n    \"add\": {\n        \"path\": \"file1.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nadd_content = json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000001.json\", add_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:32.541GMT","completionTime":"2023-07-19T03:04:32.794GMT","stageIds":[18],"jobGroup":"9","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"14c25217-29c8-47b4-ac2b-4d4c722377d5"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 9, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3700e8b3-716e-4fa7-880e-2e2870152f33","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3700e8b3-716e-4fa7-880e-2e2870152f33)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"367e4c22-c6d0-4401-90e3-6af6b6d35811"},{"cell_type":"markdown","source":["## Update some record(s)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a2dd6bfd-12bd-46fa-8337-fec9b44b87f9"},{"cell_type":"code","source":["write_parquet_file( \r\n","    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \r\n","    f\"{table_path}/file2.parquet\"\r\n","\r\n",")\r\n","add = {\r\n","    \"add\": {\r\n","        \"path\": \"file2.parquet\",\r\n","        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\r\n","        \"partitionValues\": {},\r\n","        \"modificationTime\": int(time.time_ns() / 1000000),\r\n","        \"dataChange\": True,\r\n","        \"tags\": None\r\n","    }\r\n","}\r\n","\r\n","remove = {\r\n","    \"remove\": {\r\n","        \"path\": \"file1.parquet\",\r\n","        \"deletionTimestamp\": int(time.time_ns() / 1000000),\r\n","        \"dataChange\": True,\r\n","        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\r\n","    }\r\n","}\r\n","\r\n","update_content = json.dumps(remove) + \"\\n\" + json.dumps(add)\r\n","mssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\r\n","\r\n","display(spark.read.format(\"delta\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:23.1060936Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:35.5378579Z","execution_finish_time":"2023-07-19T03:04:38.2202002Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":3946,"rowCount":2,"jobId":20,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 10:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \n    f\"{table_path}/file2.parquet\"\n\n)\nadd = {\n    \"add\": {\n        \"path\": \"file2.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nremove = {\n    \"remove\": {\n        \"path\": \"file1.parquet\",\n        \"deletionTimestamp\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\n    }\n}\n\nupdate_content = json.dumps(remove) + \"\n\" + json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path))","submissionTime":"2023-07-19T03:04:37.512GMT","completionTime":"2023-07-19T03:04:37.619GMT","stageIds":[35],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1566,"rowCount":4,"jobId":19,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 10:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \n    f\"{table_path}/file2.parquet\"\n\n)\nadd = {\n    \"add\": {\n        \"path\": \"file2.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nremove = {\n    \"remove\": {\n        \"path\": \"file1.parquet\",\n        \"deletionTimestamp\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\n    }\n}\n\nupdate_content = json.dumps(remove) + \"\n\" + json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:37.223GMT","completionTime":"2023-07-19T03:04:37.449GMT","stageIds":[33,34],"jobGroup":"10","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4324,"rowCount":50,"jobId":18,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \n    f\"{table_path}/file2.parquet\"\n\n)\nadd = {\n    \"add\": {\n        \"path\": \"file2.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nremove = {\n    \"remove\": {\n        \"path\": \"file1.parquet\",\n        \"deletionTimestamp\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\n    }\n}\n\nupdate_content = json.dumps(remove) + \"\n\" + json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 2","submissionTime":"2023-07-19T03:04:36.939GMT","completionTime":"2023-07-19T03:04:36.968GMT","stageIds":[30,31,32],"jobGroup":"10","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4324,"dataRead":1591,"rowCount":55,"jobId":17,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \n    f\"{table_path}/file2.parquet\"\n\n)\nadd = {\n    \"add\": {\n        \"path\": \"file2.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nremove = {\n    \"remove\": {\n        \"path\": \"file1.parquet\",\n        \"deletionTimestamp\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\n    }\n}\n\nupdate_content = json.dumps(remove) + \"\n\" + json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 2","submissionTime":"2023-07-19T03:04:36.425GMT","completionTime":"2023-07-19T03:04:36.923GMT","stageIds":[28,29],"jobGroup":"10","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1591,"dataRead":887,"rowCount":10,"jobId":16,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\nwrite_parquet_file( \n    {\"id\": [1, 2], \"name\": [\"first\", \"second updated\"]}, \n    f\"{table_path}/file2.parquet\"\n\n)\nadd = {\n    \"add\": {\n        \"path\": \"file2.parquet\",\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file2.parquet\")[0].size,\n        \"partitionValues\": {},\n        \"modificationTime\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"tags\": None\n    }\n}\n\nremove = {\n    \"remove\": {\n        \"path\": \"file1.parquet\",\n        \"deletionTimestamp\": int(time.time_ns() / 1000000),\n        \"dataChange\": True,\n        \"size\": mssparkutils.fs.ls(f\"{table_path}/file1.parquet\")[0].size\n    }\n}\n\nupdate_content = json.dumps(remove) + \"\n\" + json.dumps(add)\nmssparkutils.fs.put(f\"{table_path}/_delta_log/00000000000000000002.json\", update_content, overwrite=True)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Compute snapshot for version: 2","submissionTime":"2023-07-19T03:04:36.147GMT","completionTime":"2023-07-19T03:04:36.236GMT","stageIds":[27],"jobGroup":"10","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"53e9109c-bd0a-4748-9435-66cba5d10048"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 10, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"2d376f32-698a-47aa-919f-0ba1138ffa3b","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 2d376f32-698a-47aa-919f-0ba1138ffa3b)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"2e2bfc8e-590d-44d7-a4b9-ffe703c897aa"},{"cell_type":"markdown","source":["## Table folder contents\r\n","\r\n","Check table folder contents in OneLake explorer or side menu or `mssparkutils.fs.ls`\r\n","\r\n","![root](https://i.imgur.com/qqejI9P.png)\r\n","\r\n","![log](https://i.imgur.com/uL5Aadu.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b9507d8f-dc45-427d-b6b9-0d3f287bb67b"},{"cell_type":"markdown","source":["# Time travel"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd3fa6e6-52bb-4c19-a0d3-6c1c509088ce"},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:23.2774734Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:38.7699297Z","execution_finish_time":"2023-07-19T03:04:40.5984111Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":2050,"rowCount":2,"jobId":25,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 11:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path))","submissionTime":"2023-07-19T03:04:40.229GMT","completionTime":"2023-07-19T03:04:40.305GMT","stageIds":[44],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1473,"rowCount":3,"jobId":24,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:39.961GMT","completionTime":"2023-07-19T03:04:40.159GMT","stageIds":[42,43],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4319,"rowCount":50,"jobId":23,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:39.783GMT","completionTime":"2023-07-19T03:04:39.821GMT","stageIds":[39,40,41],"jobGroup":"11","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4319,"dataRead":901,"rowCount":53,"jobId":22,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:39.226GMT","completionTime":"2023-07-19T03:04:39.760GMT","stageIds":[37,38],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":901,"dataRead":639,"rowCount":6,"jobId":21,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:39.021GMT","completionTime":"2023-07-19T03:04:39.073GMT","stageIds":[36],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"78aa93f3-ace4-4156-ba7e-f2e56f661980"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 11, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"273b0d9b-a2b4-4b89-9e71-a3840e850a34","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 273b0d9b-a2b4-4b89-9e71-a3840e850a34)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"a8792f61-b9d3-4284-9a3c-f1aea89f1c9b"},{"cell_type":"markdown","source":["## Dummy/interim files effect in table folder"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"95767700-c105-4a1f-9cd8-e3dac9771c28"},{"cell_type":"code","source":["write_parquet_file( \r\n","    {\"id\": [11, 22], \"name\": [\"eleven\", \"twenty two\"]}, \r\n","    f\"{table_path}/dummy-file.parquet\"\r\n",")\r\n","\r\n","display(spark.read.format(\"delta\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:23.4063626Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:41.1257022Z","execution_finish_time":"2023-07-19T03:04:42.1108704Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":2,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":2074,"rowCount":2,"jobId":27,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 12:\nwrite_parquet_file( \n    {\"id\": [11, 22], \"name\": [\"eleven\", \"twenty two\"]}, \n    f\"{table_path}/dummy-file.parquet\"\n)\n\ndisplay(spark.read.format(\"delta\").load(table_path))","submissionTime":"2023-07-19T03:04:41.621GMT","completionTime":"2023-07-19T03:04:41.687GMT","stageIds":[47],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1150,"rowCount":1,"jobId":26,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 12:\nwrite_parquet_file( \n    {\"id\": [11, 22], \"name\": [\"eleven\", \"twenty two\"]}, \n    f\"{table_path}/dummy-file.parquet\"\n)\n\ndisplay(spark.read.format(\"delta\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:41.451GMT","completionTime":"2023-07-19T03:04:41.578GMT","stageIds":[45,46],"jobGroup":"12","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9fe23a1e-5c1e-49b4-88cb-b430c9c071f7"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 12, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"304b034d-53bd-4623-b1e0-c8f9680139c3","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 304b034d-53bd-4623-b1e0-c8f9680139c3)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"8f841ab3-522f-49fb-b918-068835dc0d30"},{"cell_type":"markdown","source":["## Vacuum the table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ba2ec138-6152-4c6a-b2b1-6ce37a1f20c3"},{"cell_type":"code","source":["from delta import DeltaTable\r\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\r\n","\r\n","# number of hours can be a floating point to allow granularity of minutes or seconds\r\n","DeltaTable.forPath(spark, table_path).vacuum(0)\r\n","\r\n","mssparkutils.fs.ls(table_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:23.6620697Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:42.6858248Z","execution_finish_time":"2023-07-19T03:04:51.3721647Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":12,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":528,"rowCount":2,"jobId":41,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:49.102GMT","completionTime":"2023-07-19T03:04:49.618GMT","stageIds":[93,94,91,95,92],"jobGroup":"13","status":"SUCCEEDED","numTasks":609,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":409,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":4,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":528,"dataRead":248,"rowCount":5,"jobId":40,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:49.013GMT","completionTime":"2023-07-19T03:04:49.075GMT","stageIds":[88,89,90,87],"jobGroup":"13","status":"SUCCEEDED","numTasks":409,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":408,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":76,"dataRead":1566,"rowCount":5,"jobId":38,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:48.072GMT","completionTime":"2023-07-19T03:04:48.903GMT","stageIds":[82,83],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":248,"dataRead":9136,"rowCount":6,"jobId":37,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:48.058GMT","completionTime":"2023-07-19T03:04:48.602GMT","stageIds":[81,79,80],"jobGroup":"13","status":"SUCCEEDED","numTasks":408,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":208,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":59,"rowCount":1,"jobId":36,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:47.877GMT","completionTime":"2023-07-19T03:04:47.904GMT","stageIds":[78,74,75,76,77],"jobGroup":"13","status":"SUCCEEDED","numTasks":410,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":409,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":4,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":59,"dataRead":263,"rowCount":4,"jobId":35,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:47.717GMT","completionTime":"2023-07-19T03:04:47.840GMT","stageIds":[70,71,72,73],"jobGroup":"13","status":"SUCCEEDED","numTasks":409,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":408,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":76,"dataRead":1566,"rowCount":5,"jobId":33,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:45.633GMT","completionTime":"2023-07-19T03:04:47.535GMT","stageIds":[66,65],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":263,"dataRead":9136,"rowCount":6,"jobId":32,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:45.567GMT","completionTime":"2023-07-19T03:04:46.839GMT","stageIds":[63,64,62],"jobGroup":"13","status":"SUCCEEDED","numTasks":408,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":208,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":11200,"rowCount":200,"jobId":31,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:45.251GMT","completionTime":"2023-07-19T03:04:45.278GMT","stageIds":[60,61,58,59],"jobGroup":"13","status":"SUCCEEDED","numTasks":409,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":408,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":11200,"dataRead":9136,"rowCount":203,"jobId":30,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:44.802GMT","completionTime":"2023-07-19T03:04:45.231GMT","stageIds":[56,57,55],"jobGroup":"13","status":"SUCCEEDED","numTasks":408,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":208,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":11209,"rowCount":200,"jobId":29,"name":"$anonfun$gc$16 at Logging.scala:61","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:44.638GMT","completionTime":"2023-07-19T03:04:44.681GMT","stageIds":[51,52,53,54],"jobGroup":"13","status":"SUCCEEDED","numTasks":409,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":408,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":13001,"dataRead":1792,"rowCount":212,"jobId":28,"name":"$anonfun$gc$16 at Logging.scala:61","description":"Job group for statement 13:\nfrom delta import DeltaTable\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n\n# number of hours can be a floating point to allow granularity of minutes or seconds\nDeltaTable.forPath(spark, table_path).vacuum(0)\n\nmssparkutils.fs.ls(table_path)","submissionTime":"2023-07-19T03:04:42.990GMT","completionTime":"2023-07-19T03:04:44.615GMT","stageIds":[48,49,50],"jobGroup":"13","status":"SUCCEEDED","numTasks":408,"numActiveTasks":0,"numCompletedTasks":408,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":408,"numActiveStages":0,"numCompletedStages":3,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"dec4dcf1-d3de-4ba3-b18d-14e9efa1c23f"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 13, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"[FileInfo(path=abfss://339a0302-9394-4cd7-af3c-44fd076923ce@onelake.dfs.fabric.microsoft.com/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/_delta_log, name=_delta_log, size=0),\n FileInfo(path=abfss://339a0302-9394-4cd7-af3c-44fd076923ce@onelake.dfs.fabric.microsoft.com/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/file2.parquet, name=file2.parquet, size=2249)]"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"89c8da91-df8d-4292-b641-e23d7a603d11"},{"cell_type":"markdown","source":["## Have we lost anything with vacuuming?"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8ee30d19-cfb7-404c-9875-a61579bbc6bc"},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:23.8008389Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:51.8639564Z","execution_finish_time":"2023-07-19T03:04:52.8567722Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":2,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":2074,"rowCount":2,"jobId":43,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 14:\ndisplay(spark.read.format(\"delta\").load(table_path))","submissionTime":"2023-07-19T03:04:52.059GMT","completionTime":"2023-07-19T03:04:52.128GMT","stageIds":[98],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1150,"rowCount":1,"jobId":42,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 14:\ndisplay(spark.read.format(\"delta\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:51.909GMT","completionTime":"2023-07-19T03:04:52.001GMT","stageIds":[96,97],"jobGroup":"14","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"83cb14f6-1458-4996-9990-2cb5f65c2a3f"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 14, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"5a94aa45-9a2c-45e3-ba2f-877d0c2700b5","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 5a94aa45-9a2c-45e3-ba2f-877d0c2700b5)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"2dde96cd-8924-4549-85f3-5cfa7c836f49"},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:04:28.9440194Z","session_start_time":null,"execution_start_time":"2023-07-19T03:04:53.3681834Z","execution_finish_time":"2023-07-19T03:04:56.1105511Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":4,"UNKNOWN":0,"FAILED":1},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":48,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 15:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path))","submissionTime":"2023-07-19T03:04:54.534GMT","completionTime":"2023-07-19T03:04:54.614GMT","stageIds":[107],"jobGroup":"15","status":"FAILED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":4,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":1,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1473,"rowCount":3,"jobId":47,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 15:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Filtering files for query","submissionTime":"2023-07-19T03:04:54.340GMT","completionTime":"2023-07-19T03:04:54.489GMT","stageIds":[105,106],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4319,"rowCount":50,"jobId":46,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:54.182GMT","completionTime":"2023-07-19T03:04:54.206GMT","stageIds":[102,103,104],"jobGroup":"15","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4319,"dataRead":901,"rowCount":53,"jobId":45,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:53.819GMT","completionTime":"2023-07-19T03:04:54.167GMT","stageIds":[100,101],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":901,"dataRead":639,"rowCount":6,"jobId":44,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\ndisplay(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(table_path)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:04:53.664GMT","completionTime":"2023-07-19T03:04:53.713GMT","stageIds":[99],"jobGroup":"15","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9159e035-8f94-4016-a4e4-fbeafef0e3ae"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 15, Finished, Available)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 4 times, most recent failure: Lost task 0.3 in stage 107.0 (TID 2044) (vm-65d08095 executor 1): java.io.FileNotFoundException: \nOperation failed: \"Not Found\", 404, HEAD, https://onelake.dfs.fabric.microsoft.com/339a0302-9394-4cd7-af3c-44fd076923ce/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/file1.parquet?upn=false&action=getStatus&timeout=90\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:149)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:584)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:764)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:400)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:897)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:897)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:366)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:330)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2682)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2618)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2617)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2617)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1190)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1190)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1190)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2870)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2812)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2801)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:958)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2342)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2363)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2382)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:542)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:495)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3881)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2876)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3871)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:562)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3869)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:183)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:97)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3869)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2876)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3097)\n\tat org.apache.spark.sql.GetRowsHelper$.getRowsInJsonString(GetRowsHelper.scala:51)\n\tat com.microsoft.spark.notebook.visualization.display$.generateTableConfig(Display.scala:403)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:256)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultInternal(Display.scala:193)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultForIPython(Display.scala:109)\n\tat com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython(Display.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: \nOperation failed: \"Not Found\", 404, HEAD, https://onelake.dfs.fabric.microsoft.com/339a0302-9394-4cd7-af3c-44fd076923ce/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/file1.parquet?upn=false&action=getStatus&timeout=90\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:149)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:584)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:764)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:400)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:897)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:897)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:366)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:330)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mversionAsOf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/notebookutils/visualization/display.py:240\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(data, summary)\u001b[0m\n\u001b[1;32m    237\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     log4jLogger \\\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay failed with error, language: python, error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, correlationId=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     duration_ms \u001b[38;5;241m=\u001b[39m ceil((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/notebookutils/visualization/display.py:218\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(data, summary)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ipython_enabled(runtime\u001b[38;5;241m.\u001b[39mhost_nbutils_version):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# pylint: disable=C0415\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m publish_display_data\n\u001b[1;32m    217\u001b[0m     publish_display_data({\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/vnd.synapse.display-widget+json\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDisplayResultForIPython\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrelation_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     })\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mgetDisplayResult(df\u001b[38;5;241m.\u001b[39m_jdf, summary))\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 4 times, most recent failure: Lost task 0.3 in stage 107.0 (TID 2044) (vm-65d08095 executor 1): java.io.FileNotFoundException: \nOperation failed: \"Not Found\", 404, HEAD, https://onelake.dfs.fabric.microsoft.com/339a0302-9394-4cd7-af3c-44fd076923ce/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/file1.parquet?upn=false&action=getStatus&timeout=90\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:149)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:584)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:764)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:400)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:897)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:897)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:366)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:330)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2682)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2618)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2617)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2617)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1190)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1190)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1190)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2870)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2812)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2801)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:958)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2342)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2363)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2382)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:542)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:495)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3881)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2876)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3871)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:562)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3869)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:183)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:97)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3869)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2876)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3097)\n\tat org.apache.spark.sql.GetRowsHelper$.getRowsInJsonString(GetRowsHelper.scala:51)\n\tat com.microsoft.spark.notebook.visualization.display$.generateTableConfig(Display.scala:403)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:256)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultInternal(Display.scala:193)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultForIPython(Display.scala:109)\n\tat com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython(Display.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: \nOperation failed: \"Not Found\", 404, HEAD, https://onelake.dfs.fabric.microsoft.com/339a0302-9394-4cd7-af3c-44fd076923ce/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/7ff7b980-28af-49ac-9545-fff16b7a8c76/file1.parquet?upn=false&action=getStatus&timeout=90\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:149)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:584)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:764)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:400)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:897)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:897)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:366)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:330)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"ab17ba7c-2d57-46c6-bd17-782c9cadbe82\",\"activityId\":\"affdda35-bc35-4065-9eba-325f313bedbf\",\"applicationId\":\"application_1689718217607_0001\",\"jobGroupId\":\"15\",\"advices\":{\"error\":1}}"}},"id":"536a3658-19b8-4405-9adf-8f11a33fd68c"},{"cell_type":"markdown","source":["## Let's see if the hisory is preserved"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fdf11339-bc88-497a-9f9a-34c8cf94a719"},{"cell_type":"code","source":["display(DeltaTable.forPath(spark, table_path).history())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:05:12.779107Z","session_start_time":null,"execution_start_time":"2023-07-19T03:05:13.3167304Z","execution_finish_time":"2023-07-19T03:05:14.3150476Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":3,"jobId":49,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 16:\ndisplay(DeltaTable.forPath(spark, table_path).history())","submissionTime":"2023-07-19T03:05:13.447GMT","completionTime":"2023-07-19T03:05:13.840GMT","stageIds":[108],"jobGroup":"16","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"25e5d76f-03b1-4fe5-a740-5a1c644b839f"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 16, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3762455a-be5b-49b8-aacc-90d36ac39bd9","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3762455a-be5b-49b8-aacc-90d36ac39bd9)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"2df5de96-46a1-486d-8411-4666b4e6d56b"},{"cell_type":"markdown","source":["Quoting [delta lake docs](https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table):\r\n","\r\n","> vacuum deletes only data files, not log files. Log files are deleted automatically and asynchronously after checkpoint operations. The default retention period of log files is 30 days, configurable through the delta.logRetentionDuration property which you set with the ALTER TABLE SET TBLPROPERTIES SQL method."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3e0e474a-b13c-4fd2-8ac6-f86d851c714f"},{"cell_type":"markdown","source":["## Accumulate lots of changes in a single table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c82a8a99-30e9-4a35-85d4-d955aecc5167"},{"cell_type":"code","source":["\r\n","from joblib import Parallel, delayed\r\n","\r\n","def add_couple_records_to_table(index):\r\n","  write_parquet_file( \r\n","    {\"id\": [index, index + 1], \"name\": [f\"item {index}\", f\"item {index + 1}\"]}, \r\n","    f\"{table_path}/new-file-{index}.parquet\"\r\n","  )\r\n","  \r\n","  version_str = f\"{index:020d}\"\r\n","  add = {\r\n","      \"add\": {\r\n","          \"path\": f\"new-file-{index}.parquet\",\r\n","          \"size\": mssparkutils.fs.ls(f\"{table_path}/new-file-{index}.parquet\")[0].size,\r\n","          \"partitionValues\": {},\r\n","          \"modificationTime\": int(time.time_ns() / 1000000),\r\n","          \"dataChange\": True,\r\n","          \"tags\": None\r\n","      }\r\n","  }\r\n","\r\n","  add_content = json.dumps(add)\r\n","  mssparkutils.fs.put(f\"{table_path}/_delta_log/{version_str}.json\", add_content, overwrite=True)\r\n","  \r\n","\r\n","PARALLELISM = 8\r\n","def run_paralell_jobs(func_to_run, items):\r\n","    tasks = [delayed(func_to_run)(x) for x in items]\r\n","    result = Parallel(n_jobs=PARALLELISM, prefer=\"threads\")(tasks)\r\n","    return result\r\n","\t\r\n","\t\r\n","_ = run_paralell_jobs(add_couple_records_to_table, range(3, 500))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"39571939-bf21-433d-84ac-3eed5c8425d3"},{"cell_type":"markdown","source":["Check delta log folder now in OneLake explorer.\r\n","\r\n","Other than the tiny files problem (which is not ) what is the impact on common queries on table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4e6e4d4d-a193-47f1-bde0-851bdfbfd627"},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(table_path).describe())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":19,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:09:31.55735Z","session_start_time":null,"execution_start_time":"2023-07-19T03:09:31.9911648Z","execution_finish_time":"2023-07-19T03:09:40.5665269Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":6,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":2091,"rowCount":16,"jobId":55,"name":"describe at NativeMethodAccessorImpl.java:0","description":"Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe())","submissionTime":"2023-07-19T03:09:39.826GMT","completionTime":"2023-07-19T03:09:39.902GMT","stageIds":[118,119],"jobGroup":"19","status":"SUCCEEDED","numTasks":17,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2091,"dataRead":1956448,"rowCount":1012,"jobId":54,"name":"describe at NativeMethodAccessorImpl.java:0","description":"Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe())","submissionTime":"2023-07-19T03:09:34.803GMT","completionTime":"2023-07-19T03:09:39.799GMT","stageIds":[117],"jobGroup":"19","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":13994,"rowCount":501,"jobId":53,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Filtering files for query","submissionTime":"2023-07-19T03:09:34.618GMT","completionTime":"2023-07-19T03:09:34.737GMT","stageIds":[115,116],"jobGroup":"19","status":"SUCCEEDED","numTasks":66,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4942,"rowCount":50,"jobId":52,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 499","submissionTime":"2023-07-19T03:09:34.309GMT","completionTime":"2023-07-19T03:09:34.331GMT","stageIds":[114,112,113],"jobGroup":"19","status":"SUCCEEDED","numTasks":67,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":66,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4942,"dataRead":138095,"rowCount":552,"jobId":51,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 499","submissionTime":"2023-07-19T03:09:33.922GMT","completionTime":"2023-07-19T03:09:34.296GMT","stageIds":[111,110],"jobGroup":"19","status":"SUCCEEDED","numTasks":66,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":138095,"dataRead":73842,"rowCount":1004,"jobId":50,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 499","submissionTime":"2023-07-19T03:09:32.354GMT","completionTime":"2023-07-19T03:09:33.794GMT","stageIds":[109],"jobGroup":"19","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"49004ec3-8e06-4070-b61c-d6d85661afc3"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 19, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"fec86a77-ebc3-406e-abf4-480539d593f5","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, fec86a77-ebc3-406e-abf4-480539d593f5)"},"metadata":{}}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"fd8a5d28-e522-4da2-a4d2-7341fd477fda"},{"cell_type":"markdown","source":["Big part of time needed to run the query will be needed to prepare table snapshot.\r\n","\r\n","![query](https://i.imgur.com/WCPYztN.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0a9653ef-ef22-4dbc-9c9b-19f17b3affc8"},{"cell_type":"code","source":["display(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\"))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":20,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:12:05.1766475Z","session_start_time":null,"execution_start_time":"2023-07-19T03:12:05.6736383Z","execution_finish_time":"2023-07-19T03:12:18.357855Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":6,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":14004,"rowCount":502,"jobId":61,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\"))","submissionTime":"2023-07-19T03:12:15.924GMT","completionTime":"2023-07-19T03:12:16.455GMT","stageIds":[129,130,131],"jobGroup":"20","status":"SUCCEEDED","numTasks":67,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":66,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":14004,"dataRead":14229,"rowCount":1004,"jobId":60,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\"))","submissionTime":"2023-07-19T03:12:15.534GMT","completionTime":"2023-07-19T03:12:15.731GMT","stageIds":[127,128],"jobGroup":"20","status":"SUCCEEDED","numTasks":66,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4942,"rowCount":50,"jobId":59,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\")): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:15.310GMT","completionTime":"2023-07-19T03:12:15.338GMT","stageIds":[125,126,124],"jobGroup":"20","status":"SUCCEEDED","numTasks":67,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":66,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4942,"dataRead":139439,"rowCount":554,"jobId":58,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\")): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:14.927GMT","completionTime":"2023-07-19T03:12:15.297GMT","stageIds":[122,123],"jobGroup":"20","status":"SUCCEEDED","numTasks":66,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":139439,"dataRead":74628,"rowCount":1008,"jobId":57,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\")): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:13.087GMT","completionTime":"2023-07-19T03:12:14.773GMT","stageIds":[121],"jobGroup":"20","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1242,"dataRead":0,"rowCount":1,"jobId":56,"name":"sql at NativeMethodAccessorImpl.java:0","description":"Job group for statement 20:\ndisplay(spark.sql(f\"INSERT INTO delta.`{table_path}` VALUES (1000000, 'item 1000000')\"))","submissionTime":"2023-07-19T03:12:12.165GMT","completionTime":"2023-07-19T03:12:12.468GMT","stageIds":[120],"jobGroup":"20","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"33322ff1-2b08-4150-ac95-1fd0d3470967"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 20, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"a62ba25e-0374-477a-86ed-cb8583897ff0","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, a62ba25e-0374-477a-86ed-cb8583897ff0)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"3231e7f7-a2bc-47ad-9616-a808307ddc2b"},{"cell_type":"markdown","source":["Checkpoint marker and parquet files are generated\r\n","\r\n","![checkpoint](https://i.imgur.com/zKjx0bi.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a5d7f7af-20aa-4884-aada-655e4b99ae06"},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(table_path).describe())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"affdda35-bc35-4065-9eba-325f313bedbf","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:12:41.0898187Z","session_start_time":null,"execution_start_time":"2023-07-19T03:12:41.5038672Z","execution_finish_time":"2023-07-19T03:12:46.7409009Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":6,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":2100,"rowCount":16,"jobId":67,"name":"describe at NativeMethodAccessorImpl.java:0","description":"Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe())","submissionTime":"2023-07-19T03:12:46.296GMT","completionTime":"2023-07-19T03:12:46.315GMT","stageIds":[141,142],"jobGroup":"21","status":"SUCCEEDED","numTasks":17,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2100,"dataRead":1028715,"rowCount":1013,"jobId":66,"name":"describe at NativeMethodAccessorImpl.java:0","description":"Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe())","submissionTime":"2023-07-19T03:12:42.888GMT","completionTime":"2023-07-19T03:12:46.286GMT","stageIds":[140],"jobGroup":"21","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":14229,"rowCount":502,"jobId":65,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Filtering files for query","submissionTime":"2023-07-19T03:12:42.759GMT","completionTime":"2023-07-19T03:12:42.837GMT","stageIds":[139,138],"jobGroup":"21","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4942,"rowCount":50,"jobId":64,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:42.509GMT","completionTime":"2023-07-19T03:12:42.531GMT","stageIds":[135,136,137],"jobGroup":"21","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4942,"dataRead":29653,"rowCount":552,"jobId":63,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:42.103GMT","completionTime":"2023-07-19T03:12:42.492GMT","stageIds":[133,134],"jobGroup":"21","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29653,"dataRead":29078,"rowCount":1004,"jobId":62,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\ndisplay(spark.read.format(\"delta\").load(table_path).describe()): Compute snapshot for version: 500","submissionTime":"2023-07-19T03:12:41.769GMT","completionTime":"2023-07-19T03:12:41.986GMT","stageIds":[132],"jobGroup":"21","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3c20c9dc-e4ca-4723-ad45-e91bd8a87feb"},"text/plain":"StatementMeta(, affdda35-bc35-4065-9eba-325f313bedbf, 21, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"b852f25d-f7f5-4028-8ed7-f578bda0f7dd","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, b852f25d-f7f5-4028-8ed7-f578bda0f7dd)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"6ec2b611-9d7c-43bc-8630-aac1ebcb149a"},{"cell_type":"markdown","source":["Table snapshot computation is much faster\r\n","\r\n","![snapshot-checkpoint](https://i.imgur.com/VRUXOiw.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"66460203-4191-44f5-a465-cc71d4110dae"},{"cell_type":"markdown","source":["## Optimise table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6e35414c-3b47-4043-95d9-6fdf978f0ff6"},{"cell_type":"code","source":["DeltaTable.forPath(spark, table_path).optimize().executeCompaction()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e03e0aaf-8a8c-4900-afe5-5a36793d5da1","statement_id":47,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T01:44:23.5941678Z","session_start_time":null,"execution_start_time":"2023-07-19T01:44:24.190833Z","execution_finish_time":"2023-07-19T01:44:32.7998216Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":6},"jobs":[{"dataWritten":0,"dataRead":4557,"rowCount":50,"jobId":104,"name":"toString at String.java:2994","description":"Delta: Job group for statement 47:\nDeltaTable.forPath(spark, table_path).optimize().executeCompaction(): Compute snapshot for version: 501","submissionTime":"2023-07-19T01:44:31.079GMT","completionTime":"2023-07-19T01:44:31.105GMT","stageIds":[212,210,211],"jobGroup":"47","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4557,"dataRead":56394,"rowCount":1053,"jobId":103,"name":"toString at String.java:2994","description":"Delta: Job group for statement 47:\nDeltaTable.forPath(spark, table_path).optimize().executeCompaction(): Compute snapshot for version: 501","submissionTime":"2023-07-19T01:44:30.613GMT","completionTime":"2023-07-19T01:44:31.062GMT","stageIds":[208,209],"jobGroup":"47","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":56394,"dataRead":99104,"rowCount":2006,"jobId":102,"name":"toString at String.java:2994","description":"Delta: Job group for statement 47:\nDeltaTable.forPath(spark, table_path).optimize().executeCompaction(): Compute snapshot for version: 501","submissionTime":"2023-07-19T01:44:30.341GMT","completionTime":"2023-07-19T01:44:30.459GMT","stageIds":[207],"jobGroup":"47","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":7585,"dataRead":10257,"rowCount":1994,"jobId":101,"name":"run at ForkJoinTask.java:1402","description":"abfss://339a0302-9394-4cd7-af3c-44fd076923ce@onelake.dfs.fabric.microsoft.com/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/ab6452af-0fd9-41ef-bc01-1641d5710142<br/>Optimizing 499 files","submissionTime":"2023-07-19T01:44:29.371GMT","completionTime":"2023-07-19T01:44:29.624GMT","stageIds":[205,206],"jobGroup":"47","status":"SUCCEEDED","numTasks":17,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10257,"dataRead":1028715,"rowCount":1994,"jobId":100,"name":"run at ForkJoinTask.java:1402","description":"abfss://339a0302-9394-4cd7-af3c-44fd076923ce@onelake.dfs.fabric.microsoft.com/ab34098a-3173-4b46-ad67-133aa4716e8b/Files/delta-lake/1-fundamentals/ab6452af-0fd9-41ef-bc01-1641d5710142<br/>Optimizing 499 files","submissionTime":"2023-07-19T01:44:25.751GMT","completionTime":"2023-07-19T01:44:29.332GMT","stageIds":[204],"jobGroup":"47","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":14390,"rowCount":502,"jobId":99,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 47:\nDeltaTable.forPath(spark, table_path).optimize().executeCompaction()","submissionTime":"2023-07-19T01:44:25.501GMT","completionTime":"2023-07-19T01:44:25.592GMT","stageIds":[202,203],"jobGroup":"47","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"114623dd-2ddf-42a4-95de-fe8cbe460dd9"},"text/plain":"StatementMeta(, e03e0aaf-8a8c-4900-afe5-5a36793d5da1, 47, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":125,"data":{"text/plain":"DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>>]"},"metadata":{}}],"execution_count":45,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"ab17ba7c-2d57-46c6-bd17-782c9cadbe82\",\"activityId\":\"e03e0aaf-8a8c-4900-afe5-5a36793d5da1\",\"applicationId\":\"application_1689707810182_0001\",\"jobGroupId\":\"47\",\"advices\":{\"info\":1}}"}},"id":"079e4364-ea2f-420b-a867-31f3f3f88916"},{"cell_type":"markdown","source":["Small files will be merged into larger ones, the top file below is a file holding all table contents.\r\n","\r\n","![compaction](https://i.imgur.com/IpCkQsu.png)\r\n","\r\n","\r\n","Last commit file shows all old acive files are removed and a single new file added\r\n","\r\n","![data](https://i.imgur.com/wdiEBfC.png)\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c2f3dfdb-6d81-4e33-9f61-af96a9bf94cd"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"3700e8b3-716e-4fa7-880e-2e2870152f33":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"first","index":1},{"0":"2","1":"second","index":2}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"2d376f32-698a-47aa-919f-0ba1138ffa3b":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"first","index":1},{"0":"2","1":"second updated","index":2}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"273b0d9b-a2b4-4b89-9e71-a3840e850a34":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"first","index":1},{"0":"2","1":"second","index":2}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"304b034d-53bd-4623-b1e0-c8f9680139c3":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"first","index":1},{"0":"2","1":"second updated","index":2}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"5a94aa45-9a2c-45e3-ba2f-877d0c2700b5":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"first","index":1},{"0":"2","1":"second updated","index":2}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"3762455a-be5b-49b8-aacc-90d36ac39bd9":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2","1":"2023-07-19 03:04:35.905","2":"NULL","3":"NULL","4":"NULL","5":"NULL","6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"NULL","11":"NULL","12":"NULL","13":"NULL","14":"NULL","index":1},{"0":"1","1":"2023-07-19 03:04:32.226","2":"NULL","3":"NULL","4":"NULL","5":"NULL","6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"NULL","11":"NULL","12":"NULL","13":"NULL","14":"NULL","index":2},{"0":"0","1":"2023-07-19 03:04:27.069","2":"NULL","3":"NULL","4":"NULL","5":"NULL","6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"NULL","11":"NULL","12":"NULL","13":"NULL","14":"NULL","index":3}],"schema":[{"key":"0","name":"version","type":"bigint"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"userId","type":"string"},{"key":"3","name":"userName","type":"string"},{"key":"4","name":"operation","type":"string"},{"key":"5","name":"operationParameters","type":"MapType(StringType,StringType,true)"},{"key":"6","name":"job","type":"StructType(StructField(jobId,StringType,true),StructField(jobName,StringType,true),StructField(runId,StringType,true),StructField(jobOwnerId,StringType,true),StructField(triggerType,StringType,true))"},{"key":"7","name":"notebook","type":"StructType(StructField(notebookId,StringType,true))"},{"key":"8","name":"clusterId","type":"string"},{"key":"9","name":"readVersion","type":"bigint"},{"key":"10","name":"isolationLevel","type":"string"},{"key":"11","name":"isBlindAppend","type":"boolean"},{"key":"12","name":"operationMetrics","type":"MapType(StringType,StringType,true)"},{"key":"13","name":"userMetadata","type":"string"},{"key":"14","name":"engineInfo","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"line","categoryFieldKeys":["1"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"fec86a77-ebc3-406e-abf4-480539d593f5":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"count","1":"996","2":"996","index":1},{"0":"mean","1":"250.99799196787149","2":"NULL","index":2},{"0":"stddev","1":"143.83649985115426","2":"NULL","index":3},{"0":"min","1":"1","2":"first","index":4},{"0":"max","1":"500","2":"second updated","index":5}],"schema":[{"key":"0","name":"summary","type":"string"},{"key":"1","name":"id","type":"string"},{"key":"2","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"b852f25d-f7f5-4028-8ed7-f578bda0f7dd":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"count","1":"997","2":"997","index":1},{"0":"mean","1":"1253.7552657973922","2":"NULL","index":2},{"0":"stddev","1":"31662.694956905056","2":"NULL","index":3},{"0":"min","1":"1","2":"first","index":4},{"0":"max","1":"1000000","2":"second updated","index":5}],"schema":[{"key":"0","name":"summary","type":"string"},{"key":"1","name":"id","type":"string"},{"key":"2","name":"name","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"default_lakehouse":"ab34098a-3173-4b46-ad67-133aa4716e8b","known_lakehouses":[{"id":"ab34098a-3173-4b46-ad67-133aa4716e8b"}],"default_lakehouse_name":"MyLake","default_lakehouse_workspace_id":"339a0302-9394-4cd7-af3c-44fd076923ce"}}},"nbformat":4,"nbformat_minor":5}