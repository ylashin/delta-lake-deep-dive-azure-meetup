{"cells":[{"cell_type":"markdown","source":["# 2 - Concurrency"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bf663def-512d-42a3-a262-c9460c5fe110"},{"cell_type":"code","source":["%pip install tenacity --quiet"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":-1,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:50:15.5543073Z","session_start_time":null,"execution_start_time":"2023-07-19T03:50:27.6208729Z","execution_finish_time":"2023-07-19T03:50:27.6210158Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d6fadc84-db8c-40e8-8ffe-a99dd0a46dd1"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, -1, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":36,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-83b1a4ec-c3bc-4a52-abb7-7092cdccbb2a/bin/python -m pip install --upgrade pip\u001b[0m\r\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":36,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":36,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fadee00b-e29e-42e6-b90a-854a722b075a"},{"cell_type":"markdown","source":["## Imports and data classes"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"17ae79d5-3728-4702-bf91-ce3c6154d401"},{"cell_type":"code","source":["import os\n","import uuid\n","from joblib import Parallel, delayed\n","from dataclasses import dataclass\n","from delta.tables import *\n","\n","@dataclass\n","class TaskResult:\n","    task_id: int\n","    is_successful: bool\n","    error_message: str\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":44,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:50:45.123672Z","session_start_time":null,"execution_start_time":"2023-07-19T03:50:55.2965821Z","execution_finish_time":"2023-07-19T03:50:55.643679Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ea5455bb-d11b-4751-a029-7faf9ccdc150"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 44, Finished, Available)"},"metadata":{}}],"execution_count":37,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e7933ddb-5697-4314-8c36-c28b121365e2"},{"cell_type":"markdown","source":["## File system preparation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6ff4c19d-bc8a-4938-a224-57f6fabda0f0"},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","\n","table_path_root = \"Files/delta-lake/2-concurrency\"\n","\n","table_append = f\"{table_path_root}/append\"\n","table_merge = f\"{table_path_root}/merge\"\n","table_merge_retry = f\"{table_path_root}/merge-retry\"\n","table_partitioned = f\"{table_path_root}/partitioned\"\n","\n","if mssparkutils.fs.exists(table_path_root):\n","    mssparkutils.fs.rm(table_path_root, True)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":45,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:50:47.3216657Z","session_start_time":null,"execution_start_time":"2023-07-19T03:50:56.1464513Z","execution_finish_time":"2023-07-19T03:50:57.1731399Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0b0ef415-ef11-4fe5-8aa3-91801831ff95"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 45, Finished, Available)"},"metadata":{}}],"execution_count":38,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f84dec89-e425-406b-8d47-c524a1d4a15b"},{"cell_type":"markdown","source":["## Data preparation\n","\n","Good option is Databricks Labs [Data Generator](https://github.com/databrickslabs/dbldatagen)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0de36f78-932d-4197-bfd5-b1da077700f9"},{"cell_type":"code","source":["def get_data_frame(spark_session=spark, start=0, end=1000_000):\n","    df = spark_session.sql(f\"\"\"\n","    SELECT\n","        id, CONCAT('Record ', id) as name, CAST(RAND() * 1000 AS INT) as value\n","    FROM\n","        RANGE({start}, {end})\n","    \"\"\")\n","\n","    return df\n","\n","display(get_data_frame(end=5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":46,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:51:07.1554008Z","session_start_time":null,"execution_start_time":"2023-07-19T03:51:07.5336764Z","execution_finish_time":"2023-07-19T03:51:07.929431Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":5,"jobId":235,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 46:\ndef get_data_frame(spark_session=spark, start=0, end=1000_000):\n    df = spark_session.sql(f\"\"\"\n    SELECT\n        id, CONCAT('Record ', id) as name, CAST(RAND() * 1000 AS INT) as value\n    FROM\n        RANGE({start}, {end})\n    \"\"\")\n\n    return df\n\ndisplay(get_data_frame(end=5))","submissionTime":"2023-07-19T03:51:07.517GMT","completionTime":"2023-07-19T03:51:07.562GMT","stageIds":[340],"jobGroup":"46","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9f78ebf3-4706-4164-a32b-323cfac6b146"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 46, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"0835fc3f-c2ff-4ccd-9618-e51ce04890a7","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 0835fc3f-c2ff-4ccd-9618-e51ce04890a7)"},"metadata":{}}],"execution_count":39,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"f1635976-dfd0-4253-ae90-24df3b3f2593"},{"cell_type":"code","source":["def create_table(table_path):\n","    if mssparkutils.fs.exists(table_path):\n","        mssparkutils.fs.rm(table_path, True)\n","    \n","    df = get_data_frame()\n","\n","    print(f\"Creating table at: {table_path}\")\n","    df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","\n","    display(df.limit(5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":47,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:51:19.2607299Z","session_start_time":null,"execution_start_time":"2023-07-19T03:51:19.6623088Z","execution_finish_time":"2023-07-19T03:51:20.0082005Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c10d81c9-4641-45a3-a9d7-e93c05f13b21"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 47, Finished, Available)"},"metadata":{}}],"execution_count":40,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5c2e2d53-a87e-4df9-aa89-579250462ed4"},{"cell_type":"code","source":["PARALLELISM = 4\n","def run_paralell_jobs(func_to_run, items):\n","    tasks = [delayed(func_to_run)(x) for x in items]\n","    result = Parallel(n_jobs=PARALLELISM, prefer=\"threads\")(tasks)\n","    return result"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":48,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:51:37.883786Z","session_start_time":null,"execution_start_time":"2023-07-19T03:51:38.2745048Z","execution_finish_time":"2023-07-19T03:51:38.6210406Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6fd3d04c-91ee-427e-98b9-acf346206bee"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 48, Finished, Available)"},"metadata":{}}],"execution_count":41,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4be09648-9355-4418-8c02-3dc157257ee3"},{"cell_type":"markdown","source":["## Blind Appends"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3a4a66be-dd21-48e8-853a-a79fa5a54949"},{"cell_type":"code","source":["create_table(table_append)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":49,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:51:52.8139844Z","session_start_time":null,"execution_start_time":"2023-07-19T03:51:53.2068829Z","execution_finish_time":"2023-07-19T03:51:55.9701885Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":7,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":242,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 49:\ncreate_table(table_append)","submissionTime":"2023-07-19T03:51:55.452GMT","completionTime":"2023-07-19T03:51:55.510GMT","stageIds":[350,349],"jobGroup":"49","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1250,"dataRead":0,"rowCount":80,"jobId":241,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 49:\ncreate_table(table_append)","submissionTime":"2023-07-19T03:51:55.410GMT","completionTime":"2023-07-19T03:51:55.438GMT","stageIds":[348],"jobGroup":"49","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4431,"rowCount":50,"jobId":240,"name":"toString at String.java:2994","description":"Delta: Job group for statement 49:\ncreate_table(table_append): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:51:55.335GMT","completionTime":"2023-07-19T03:51:55.356GMT","stageIds":[347,345,346],"jobGroup":"49","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4431,"dataRead":5047,"rowCount":61,"jobId":239,"name":"toString at String.java:2994","description":"Delta: Job group for statement 49:\ncreate_table(table_append): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:51:55.003GMT","completionTime":"2023-07-19T03:51:55.321GMT","stageIds":[343,344],"jobGroup":"49","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5047,"dataRead":4121,"rowCount":22,"jobId":238,"name":"toString at String.java:2994","description":"Delta: Job group for statement 49:\ncreate_table(table_append): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:51:54.838GMT","completionTime":"2023-07-19T03:51:54.885GMT","stageIds":[342],"jobGroup":"49","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":237,"name":"","description":"Job group for statement 49:\ncreate_table(table_append)","submissionTime":"2023-07-19T03:51:54.206GMT","completionTime":"2023-07-19T03:51:54.206GMT","stageIds":[],"jobGroup":"49","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":13381001,"dataRead":0,"rowCount":2000000,"jobId":236,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 49:\ncreate_table(table_append)","submissionTime":"2023-07-19T03:51:53.352GMT","completionTime":"2023-07-19T03:51:54.093GMT","stageIds":[341],"jobGroup":"49","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6aa578e8-acfd-4a65-a286-238c4629f371"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 49, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating table at: Files/delta-lake/2-concurrency/append\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"654a07c8-87b3-42eb-8bde-db1d696e07ed","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 654a07c8-87b3-42eb-8bde-db1d696e07ed)"},"metadata":{}}],"execution_count":42,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5f200579-5070-4f30-b5d5-9f8eb37e2f4d"},{"cell_type":"code","source":["def append_table(task):\n","  print(f\"Starting task: {task}\")\n","\n","  dfUpdates = get_data_frame(start=task*10_000, end=(task+1)*10_000)\n","  (\n","    dfUpdates\n","      .write\n","      .format(\"delta\")\n","      .option(\"userMetadata\", f\"commit from task {task}\")\n","      .mode(\"append\")\n","      .save(table_append)\n","  )\n","\n","  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n","  return TaskResult(task, True, None)\n","\n","run_paralell_jobs(append_table, range(4))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":50,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:51:59.3691356Z","session_start_time":null,"execution_start_time":"2023-07-19T03:51:59.7663431Z","execution_finish_time":"2023-07-19T03:52:05.1686105Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":4,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":4509,"rowCount":50,"jobId":249,"name":"toString at String.java:2994","description":"Delta: Job group for statement 50:\ndef append_table(task):\n  print(f\"Starting task: {task}\")\n\n  dfUpdates = get_data_frame(start=task*10_000, end=(task+1)*10_000)\n  (\n    dfUpdates\n      .write\n      .format(\"delta\")\n      .option(\"userMetadata\", f\"commit from task {task}\")\n      .mode(\"append\")\n      .save(table_append)\n  )\n\n  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(append_table, range(4)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:52:01.366GMT","completionTime":"2023-07-19T03:52:01.391GMT","stageIds":[358,359,360],"jobGroup":"50","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4509,"dataRead":9795,"rowCount":70,"jobId":248,"name":"toString at String.java:2994","description":"Delta: Job group for statement 50:\ndef append_table(task):\n  print(f\"Starting task: {task}\")\n\n  dfUpdates = get_data_frame(start=task*10_000, end=(task+1)*10_000)\n  (\n    dfUpdates\n      .write\n      .format(\"delta\")\n      .option(\"userMetadata\", f\"commit from task {task}\")\n      .mode(\"append\")\n      .save(table_append)\n  )\n\n  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(append_table, range(4)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:52:00.925GMT","completionTime":"2023-07-19T03:52:01.353GMT","stageIds":[356,357],"jobGroup":"50","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":9795,"dataRead":7692,"rowCount":40,"jobId":247,"name":"toString at String.java:2994","description":"Delta: Job group for statement 50:\ndef append_table(task):\n  print(f\"Starting task: {task}\")\n\n  dfUpdates = get_data_frame(start=task*10_000, end=(task+1)*10_000)\n  (\n    dfUpdates\n      .write\n      .format(\"delta\")\n      .option(\"userMetadata\", f\"commit from task {task}\")\n      .mode(\"append\")\n      .save(table_append)\n  )\n\n  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(append_table, range(4)): Compute snapshot for version: 1","submissionTime":"2023-07-19T03:52:00.762GMT","completionTime":"2023-07-19T03:52:00.810GMT","stageIds":[355],"jobGroup":"50","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":166746,"dataRead":0,"rowCount":20000,"jobId":243,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 50:\ndef append_table(task):\n  print(f\"Starting task: {task}\")\n\n  dfUpdates = get_data_frame(start=task*10_000, end=(task+1)*10_000)\n  (\n    dfUpdates\n      .write\n      .format(\"delta\")\n      .option(\"userMetadata\", f\"commit from task {task}\")\n      .mode(\"append\")\n      .save(table_append)\n  )\n\n  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(append_table, range(4))","submissionTime":"2023-07-19T03:51:59.838GMT","completionTime":"2023-07-19T03:52:00.230GMT","stageIds":[351],"jobGroup":"50","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c74b45d6-5641-4ef8-8802-9aa5e08e664a"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 50, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting task: 0\nStarting task: 1\nStarting task: 2\nStarting task: 3\nProcessed task: 0 for records with ids 0 till 9999\nProcessed task: 1 for records with ids 10000 till 19999\nProcessed task: 2 for records with ids 20000 till 29999\nProcessed task: 3 for records with ids 30000 till 39999\n"]},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"[TaskResult(task_id=0, is_successful=True, error_message=None),\n TaskResult(task_id=1, is_successful=True, error_message=None),\n TaskResult(task_id=2, is_successful=True, error_message=None),\n TaskResult(task_id=3, is_successful=True, error_message=None)]"},"metadata":{}}],"execution_count":43,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"42ddec91-cf67-474f-b44f-ba713a21a660"},{"cell_type":"code","source":["spark.read.format(\"delta\").load(table_append).count()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":51,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:52:09.2830851Z","session_start_time":null,"execution_start_time":"2023-07-19T03:52:09.6907824Z","execution_finish_time":"2023-07-19T03:52:10.6720082Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":2,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":3048,"rowCount":50,"jobId":260,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 51:\nspark.read.format(\"delta\").load(table_append).count()","submissionTime":"2023-07-19T03:52:09.969GMT","completionTime":"2023-07-19T03:52:09.986GMT","stageIds":[383,381,382],"jobGroup":"51","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3048,"dataRead":11997,"rowCount":92,"jobId":259,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 51:\nspark.read.format(\"delta\").load(table_append).count()","submissionTime":"2023-07-19T03:52:09.796GMT","completionTime":"2023-07-19T03:52:09.949GMT","stageIds":[379,380],"jobGroup":"51","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"687926a1-2d74-478e-ba7d-ef928423cf84"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 51, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"1040000"},"metadata":{}}],"execution_count":44,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b22ce2ba-abf0-43ae-9dea-781b98f3861c"},{"cell_type":"code","source":["display(DeltaTable.forPath(spark, table_append).history())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":52,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:52:13.040645Z","session_start_time":null,"execution_start_time":"2023-07-19T03:52:13.4473296Z","execution_finish_time":"2023-07-19T03:52:14.460272Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":5,"jobId":261,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 52:\ndisplay(DeltaTable.forPath(spark, table_append).history())","submissionTime":"2023-07-19T03:52:13.553GMT","completionTime":"2023-07-19T03:52:13.618GMT","stageIds":[384],"jobGroup":"52","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"72e91c0a-cd1a-4914-a52a-b6a034d57eac"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 52, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"449233e8-5e4f-4af3-a2f0-861d72469456","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 449233e8-5e4f-4af3-a2f0-861d72469456)"},"metadata":{}}],"execution_count":45,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"0d85c98e-169c-47cd-9f6e-5683e9f84c04"},{"cell_type":"markdown","source":["## Concurrent merges"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d62f9fb6-57bf-4d9a-9673-7020160d7cd5"},{"cell_type":"code","source":["create_table(table_merge)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":53,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:52:26.1481618Z","session_start_time":null,"execution_start_time":"2023-07-19T03:52:26.5731373Z","execution_finish_time":"2023-07-19T03:52:29.3198522Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":7,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":268,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 53:\ncreate_table(table_merge)","submissionTime":"2023-07-19T03:52:28.475GMT","completionTime":"2023-07-19T03:52:28.523GMT","stageIds":[393,394],"jobGroup":"53","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1258,"dataRead":0,"rowCount":80,"jobId":267,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 53:\ncreate_table(table_merge)","submissionTime":"2023-07-19T03:52:28.437GMT","completionTime":"2023-07-19T03:52:28.459GMT","stageIds":[392],"jobGroup":"53","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4431,"rowCount":50,"jobId":266,"name":"toString at String.java:2994","description":"Delta: Job group for statement 53:\ncreate_table(table_merge): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:52:28.367GMT","completionTime":"2023-07-19T03:52:28.388GMT","stageIds":[389,390,391],"jobGroup":"53","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4431,"dataRead":5021,"rowCount":61,"jobId":265,"name":"toString at String.java:2994","description":"Delta: Job group for statement 53:\ncreate_table(table_merge): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:52:28.042GMT","completionTime":"2023-07-19T03:52:28.354GMT","stageIds":[387,388],"jobGroup":"53","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5021,"dataRead":4121,"rowCount":22,"jobId":264,"name":"toString at String.java:2994","description":"Delta: Job group for statement 53:\ncreate_table(table_merge): Compute snapshot for version: 0","submissionTime":"2023-07-19T03:52:27.881GMT","completionTime":"2023-07-19T03:52:27.925GMT","stageIds":[386],"jobGroup":"53","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":263,"name":"","description":"Job group for statement 53:\ncreate_table(table_merge)","submissionTime":"2023-07-19T03:52:27.345GMT","completionTime":"2023-07-19T03:52:27.345GMT","stageIds":[],"jobGroup":"53","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":13381001,"dataRead":0,"rowCount":2000000,"jobId":262,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 53:\ncreate_table(table_merge)","submissionTime":"2023-07-19T03:52:26.687GMT","completionTime":"2023-07-19T03:52:27.234GMT","stageIds":[385],"jobGroup":"53","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"eafca06f-65ba-4b78-83ba-004cbeda1136"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 53, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating table at: Files/delta-lake/2-concurrency/merge\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"9c1279eb-78a6-44ef-a0b3-8fe505f44855","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 9c1279eb-78a6-44ef-a0b3-8fe505f44855)"},"metadata":{}}],"execution_count":46,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"73626d76-acfd-4d76-9a54-339b249ea2de"},{"cell_type":"code","source":["def merge_into_table(task):\n","  print(f\"Starting task: {task}\")\n","  \n","  spark_clone = spark.newSession()\n","  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into table task: {task}\")\n","\n","  dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n","  dfUpdates.createOrReplaceTempView(f\"updates\")  # No risk of using same view name as we have separate Spark sessions\n","\n","  try:\n","    spark_clone.sql(f\"\"\"\n","        MERGE INTO delta.`{table_merge}` target USING updates source\n","        ON target.id = source.id\n","        WHEN MATCHED \n","        THEN UPDATE SET target.value = source.value;\n","    \"\"\")\n","  except Exception as ex:\n","    ex_type = type(ex)\n","    print(f\"Task {task} failed with exception of type: {ex_type}\")\n","    return TaskResult(task, False, str(ex))\n","  \n","  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n","\n","  return TaskResult(task, True, None)\n","\n","result = run_paralell_jobs(merge_into_table, range(4))\n","print(\"-\" * 100)\n","print(result)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":54,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:52:58.3389215Z","session_start_time":null,"execution_start_time":"2023-07-19T03:52:58.7147245Z","execution_finish_time":"2023-07-19T03:53:13.6946333Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7081c41c-8edd-431a-bd9e-899197538e91"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 54, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting task: 0\nStarting task: 1\nStarting task: 2\nStarting task: 3\nProcessed task: 0 for records with ids 0 till 9999\nTask 2 failed with exception of type: <class 'delta.exceptions.ConcurrentAppendException'>\nTask 3 failed with exception of type: <class 'delta.exceptions.ConcurrentAppendException'>\nTask 1 failed with exception of type: <class 'delta.exceptions.ConcurrentAppendException'>\n----------------------------------------------------------------------------------------------------\n[TaskResult(task_id=0, is_successful=True, error_message=None), TaskResult(task_id=1, is_successful=False, error_message='Files were added to the root of the table by a concurrent update. Please try the operation again.\\nConflicting commit: {\"timestamp\":1689738786969,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":(target.id = source.id),\"matchedPredicates\":[{\"actionType\":\"update\"}],\"notMatchedPredicates\":[]},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numTargetRowsCopied\":\"115000\",\"numTargetRowsDeleted\":\"0\",\"numTargetFilesAdded\":\"2\",\"executionTimeMs\":\"7435\",\"numTargetRowsInserted\":\"0\",\"unmodifiedRewriteTimeMs\":\"3594\",\"scanTimeMs\":\"3586\",\"numTargetRowsUpdated\":\"10000\",\"numOutputRows\":\"125000\",\"numTargetChangeFilesAdded\":\"0\",\"numSourceRows\":\"10000\",\"numTargetFilesRemoved\":\"1\",\"rewriteTimeMs\":\"3378\"},\"userMetadata\":\"merge into table task: 0\",\"tags\":{\"VORDER\":\"true\"},\"engineInfo\":\"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4\",\"txnId\":\"af28b672-7f66-444e-b90d-bc32eb5430e6\"}\\nRefer to https://docs.delta.io/latest/concurrency-control.html for more details.'), TaskResult(task_id=2, is_successful=False, error_message='Files were added to the root of the table by a concurrent update. Please try the operation again.\\nConflicting commit: {\"timestamp\":1689738786969,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":(target.id = source.id),\"matchedPredicates\":[{\"actionType\":\"update\"}],\"notMatchedPredicates\":[]},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numTargetRowsCopied\":\"115000\",\"numTargetRowsDeleted\":\"0\",\"numTargetFilesAdded\":\"2\",\"executionTimeMs\":\"7435\",\"numTargetRowsInserted\":\"0\",\"unmodifiedRewriteTimeMs\":\"3594\",\"scanTimeMs\":\"3586\",\"numTargetRowsUpdated\":\"10000\",\"numOutputRows\":\"125000\",\"numTargetChangeFilesAdded\":\"0\",\"numSourceRows\":\"10000\",\"numTargetFilesRemoved\":\"1\",\"rewriteTimeMs\":\"3378\"},\"userMetadata\":\"merge into table task: 0\",\"tags\":{\"VORDER\":\"true\"},\"engineInfo\":\"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4\",\"txnId\":\"af28b672-7f66-444e-b90d-bc32eb5430e6\"}\\nRefer to https://docs.delta.io/latest/concurrency-control.html for more details.'), TaskResult(task_id=3, is_successful=False, error_message='Files were added to the root of the table by a concurrent update. Please try the operation again.\\nConflicting commit: {\"timestamp\":1689738786969,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":(target.id = source.id),\"matchedPredicates\":[{\"actionType\":\"update\"}],\"notMatchedPredicates\":[]},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numTargetRowsCopied\":\"115000\",\"numTargetRowsDeleted\":\"0\",\"numTargetFilesAdded\":\"2\",\"executionTimeMs\":\"7435\",\"numTargetRowsInserted\":\"0\",\"unmodifiedRewriteTimeMs\":\"3594\",\"scanTimeMs\":\"3586\",\"numTargetRowsUpdated\":\"10000\",\"numOutputRows\":\"125000\",\"numTargetChangeFilesAdded\":\"0\",\"numSourceRows\":\"10000\",\"numTargetFilesRemoved\":\"1\",\"rewriteTimeMs\":\"3378\"},\"userMetadata\":\"merge into table task: 0\",\"tags\":{\"VORDER\":\"true\"},\"engineInfo\":\"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4\",\"txnId\":\"af28b672-7f66-444e-b90d-bc32eb5430e6\"}\\nRefer to https://docs.delta.io/latest/concurrency-control.html for more details.')]\n"]}],"execution_count":47,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"59ed5580-beb8-4a94-92e6-6a5e1513fa55"},{"cell_type":"code","source":["display(DeltaTable.forPath(spark, table_merge).history())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":55,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T03:53:53.6905624Z","session_start_time":null,"execution_start_time":"2023-07-19T03:53:54.103474Z","execution_finish_time":"2023-07-19T03:53:55.1030613Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":2,"jobId":300,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 55:\ndisplay(DeltaTable.forPath(spark, table_merge).history())","submissionTime":"2023-07-19T03:53:54.276GMT","completionTime":"2023-07-19T03:53:54.345GMT","stageIds":[436],"jobGroup":"55","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"aff36b9b-fb7d-49bd-98bc-2fd11af54249"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 55, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"7c1cad5d-db1a-40b5-a95e-0358a1fb5642","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 7c1cad5d-db1a-40b5-a95e-0358a1fb5642)"},"metadata":{}}],"execution_count":48,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"9549309f-61aa-4ad8-a30a-216dc586954a"},{"cell_type":"markdown","source":["## Concurrent merges with retry"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"680984ce-bb4a-40e2-b334-2b03ce9b4993"},{"cell_type":"code","source":["create_table(table_merge_retry)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":61,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:03:42.5968833Z","session_start_time":null,"execution_start_time":"2023-07-19T04:03:43.0521582Z","execution_finish_time":"2023-07-19T04:03:45.8890642Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":7,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":390,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 61:\ncreate_table(table_merge_retry)","submissionTime":"2023-07-19T04:03:45.231GMT","completionTime":"2023-07-19T04:03:45.248GMT","stageIds":[562,563],"jobGroup":"61","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":389,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 61:\ncreate_table(table_merge_retry)","submissionTime":"2023-07-19T04:03:45.193GMT","completionTime":"2023-07-19T04:03:45.217GMT","stageIds":[561],"jobGroup":"61","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":388,"name":"toString at String.java:2994","description":"Delta: Job group for statement 61:\ncreate_table(table_merge_retry): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:03:45.124GMT","completionTime":"2023-07-19T04:03:45.145GMT","stageIds":[560,558,559],"jobGroup":"61","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":387,"name":"toString at String.java:2994","description":"Delta: Job group for statement 61:\ncreate_table(table_merge_retry): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:03:44.799GMT","completionTime":"2023-07-19T04:03:45.111GMT","stageIds":[556,557],"jobGroup":"61","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":386,"name":"toString at String.java:2994","description":"Delta: Job group for statement 61:\ncreate_table(table_merge_retry): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:03:44.657GMT","completionTime":"2023-07-19T04:03:44.699GMT","stageIds":[555],"jobGroup":"61","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":385,"name":"","description":"Job group for statement 61:\ncreate_table(table_merge_retry)","submissionTime":"2023-07-19T04:03:44.093GMT","completionTime":"2023-07-19T04:03:44.093GMT","stageIds":[],"jobGroup":"61","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":384,"name":"save at <unknown>:0","description":"Job group for statement 61:\ncreate_table(table_merge_retry)","submissionTime":"2023-07-19T04:03:43.264GMT","completionTime":"2023-07-19T04:03:43.990GMT","stageIds":[554],"jobGroup":"61","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"851e5a1b-164c-4d94-8ad5-7f54940dc17e"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 61, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating table at: Files/delta-lake/2-concurrency/merge-retry\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"d497164b-60b1-49d3-8e64-97cd52987e13","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, d497164b-60b1-49d3-8e64-97cd52987e13)"},"metadata":{}}],"execution_count":54,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"f6aa1e34-1f8b-4b68-84b1-5056db25d0f1"},{"cell_type":"code","source":["from tenacity import retry, stop_after_attempt, wait_random\n","from delta.exceptions import ConcurrentAppendException\n","\n","MAX_ATTEMPTS = 5\n","WAIT_TIME_MIN = 1\n","WAIT_TIME_MAX = 10\n","\n","def merge_into_table_with_retry(task):\n","  print(f\"Starting task: {task}\")\n","\n","  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n","  def merge_it(task):\n","\n","    print(f\"merge_it function is called for task: {task}\")\n","\n","\n","    spark_clone = spark.newSession()\n","    commit_message = f\"merge into table with retry task {task}\"\n","    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n","\n","    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n","    dfUpdates.createOrReplaceTempView(f\"updates\")\n","    \n","    spark_clone.sql(f\"\"\"\n","        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n","        ON target.id = source.id\n","        WHEN MATCHED \n","        THEN UPDATE SET target.value = source.value;\n","    \"\"\")\n","\n","  # No 100% guarantee that retry will prevent concurrency exceptions but ry clause is skipped here for demo purposes\n","  merge_it(task)\n","  \n","  print(f\"Processed task: {task} for records with ids {task*10_000} till {(task+1)*10_000 - 1}\")\n","  return TaskResult(task, True, None)\n","\n","run_paralell_jobs(merge_into_table_with_retry, range(4))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":62,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:03:49.6367361Z","session_start_time":null,"execution_start_time":"2023-07-19T04:03:50.091694Z","execution_finish_time":"2023-07-19T04:04:29.268072Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":18,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":465,"name":"toString at String.java:2994","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Compute snapshot for version: 4","submissionTime":"2023-07-19T04:04:28.519GMT","completionTime":"2023-07-19T04:04:28.540GMT","stageIds":[665,666,667],"jobGroup":"62","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":464,"name":"toString at String.java:2994","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Compute snapshot for version: 4","submissionTime":"2023-07-19T04:04:28.233GMT","completionTime":"2023-07-19T04:04:28.505GMT","stageIds":[663,664],"jobGroup":"62","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":463,"name":"toString at String.java:2994","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Compute snapshot for version: 4","submissionTime":"2023-07-19T04:04:27.864GMT","completionTime":"2023-07-19T04:04:28.093GMT","stageIds":[662],"jobGroup":"62","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":462,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged data full update","submissionTime":"2023-07-19T04:04:26.871GMT","completionTime":"2023-07-19T04:04:27.306GMT","stageIds":[661],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":461,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged no shuffle data","submissionTime":"2023-07-19T04:04:26.822GMT","completionTime":"2023-07-19T04:04:27.332GMT","stageIds":[660],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":459,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:24.820GMT","completionTime":"2023-07-19T04:04:26.695GMT","stageIds":[658,657],"jobGroup":"62","status":"SUCCEEDED","numTasks":509,"numActiveTasks":0,"numCompletedTasks":500,"numSkippedTasks":9,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":500,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":458,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:24.593GMT","completionTime":"2023-07-19T04:04:24.785GMT","stageIds":[656],"jobGroup":"62","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":9,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":9,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":456,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:24.322GMT","completionTime":"2023-07-19T04:04:24.402GMT","stageIds":[653,654],"jobGroup":"62","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":441,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged data full update","submissionTime":"2023-07-19T04:04:13.527GMT","completionTime":"2023-07-19T04:04:14.367GMT","stageIds":[630],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":439,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged no shuffle data","submissionTime":"2023-07-19T04:04:11.765GMT","completionTime":"2023-07-19T04:04:14.224GMT","stageIds":[628],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":435,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:09.180GMT","completionTime":"2023-07-19T04:04:11.482GMT","stageIds":[622,623],"jobGroup":"62","status":"SUCCEEDED","numTasks":508,"numActiveTasks":0,"numCompletedTasks":500,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":500,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":431,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:08.639GMT","completionTime":"2023-07-19T04:04:09.124GMT","stageIds":[617],"jobGroup":"62","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":426,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:04:06.925GMT","completionTime":"2023-07-19T04:04:08.276GMT","stageIds":[611,612],"jobGroup":"62","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":417,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged data full update","submissionTime":"2023-07-19T04:04:00.914GMT","completionTime":"2023-07-19T04:04:01.785GMT","stageIds":[597],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":415,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Writing merged no shuffle data","submissionTime":"2023-07-19T04:03:59.071GMT","completionTime":"2023-07-19T04:04:01.601GMT","stageIds":[595],"jobGroup":"62","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":411,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:03:56.316GMT","completionTime":"2023-07-19T04:03:58.781GMT","stageIds":[589,590],"jobGroup":"62","status":"SUCCEEDED","numTasks":508,"numActiveTasks":0,"numCompletedTasks":500,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":500,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":406,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:03:55.964GMT","completionTime":"2023-07-19T04:03:56.257GMT","stageIds":[584],"jobGroup":"62","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":398,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 62:\nfrom tenacity import retry, stop_after_attempt, wait_random\nfrom delta.exceptions import ConcurrentAppendException\n\nMAX_ATTEMPTS = 5\nWAIT_TIME_MIN = 1\nWAIT_TIME_MAX = 10\n\ndef merge_into_table_with_retry(task):\n  print(f\"Starting task: {task}\")\n\n  @retry(stop=stop_after_attempt(MAX_ATTEMPTS), wait=wait_random(min=WAIT_TIME_MIN, max=WAIT_TIME_MAX))\n  def merge_it(task):\n\n    print(f\"merge_it function is called for task: {task}\")\n\n\n    spark_clone = spark.newSession()\n    commit_message = f\"merge into table with retry task {task}\"\n    spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_message)\n\n    dfUpdates = get_data_frame(spark_clone, start=task*10_000, end=(task+1)*10_000)\n    dfUpdates.createOrReplaceTempView(f\"updates\")\n    \n    spark_clone.sql(f\"\"\"\n        MERGE INTO delta.`{table_merge_retry}` target USING updates source\n        ON target.id = source.id\n        WHEN MATCHED \n        THEN UPDATE SET target.value = source.value;\n    \"\"\")\n\n  # No 100% guarantee that retry will pre...: Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:03:51.310GMT","completionTime":"2023-07-19T04:03:53.699GMT","stageIds":[574,575],"jobGroup":"62","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e074f5d2-eb48-406c-865e-b65e49eb0238"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 62, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting task: 0\nmerge_it function is called for task: 0\nStarting task: 1\nmerge_it function is called for task: 1\nStarting task: 2\nmerge_it function is called for task: 2\nStarting task: 3\nmerge_it function is called for task: 3\nProcessed task: 3 for records with ids 30000 till 39999\nmerge_it function is called for task: 0\nmerge_it function is called for task: 1\nmerge_it function is called for task: 2\nProcessed task: 0 for records with ids 0 till 9999\nmerge_it function is called for task: 2\nmerge_it function is called for task: 1\nProcessed task: 2 for records with ids 20000 till 29999\nProcessed task: 1 for records with ids 10000 till 19999\n"]},{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"[TaskResult(task_id=0, is_successful=True, error_message=None),\n TaskResult(task_id=1, is_successful=True, error_message=None),\n TaskResult(task_id=2, is_successful=True, error_message=None),\n TaskResult(task_id=3, is_successful=True, error_message=None)]"},"metadata":{}}],"execution_count":55,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"611f9f47-5719-4744-bec0-062a9491a5ba"},{"cell_type":"code","source":["display(DeltaTable.forPath(spark, table_merge_retry).history())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":63,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:04:34.1933999Z","session_start_time":null,"execution_start_time":"2023-07-19T04:04:34.59773Z","execution_finish_time":"2023-07-19T04:04:35.6065847Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":466,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 63:\ndisplay(DeltaTable.forPath(spark, table_merge_retry).history())","submissionTime":"2023-07-19T04:04:34.737GMT","completionTime":"2023-07-19T04:04:34.814GMT","stageIds":[668],"jobGroup":"63","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7c6e9bff-f113-45fb-8172-4742c4dc4d57"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 63, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"2558c37f-07a7-4329-a324-003a19367ab2","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 2558c37f-07a7-4329-a324-003a19367ab2)"},"metadata":{}}],"execution_count":56,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"c42dc4a6-dc9a-4a3e-840f-17f7030c2fa9"},{"cell_type":"markdown","source":["## Use Partitions"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8a9aa4e1-d037-4053-a839-666e84177442"},{"cell_type":"code","source":["def get_data_frame_with_year_column(spark_session=spark, start=0, end=1000_000):\n","    df = spark_session.sql(f\"\"\"\n","    SELECT\n","        id,\n","        CONCAT('Record ', id) as name,\n","        (id % 4) + 2020 as year,\n","        CAST(RAND() * 1000 AS INT) as value\n","    FROM\n","        RANGE({start}, {end})\n","    \"\"\")\n","\n","    return df\n","\n","display(get_data_frame_with_year_column(end=10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":64,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:04:40.1029661Z","session_start_time":null,"execution_start_time":"2023-07-19T04:04:40.527007Z","execution_finish_time":"2023-07-19T04:04:40.9276708Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":467,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 64:\ndef get_data_frame_with_year_column(spark_session=spark, start=0, end=1000_000):\n    df = spark_session.sql(f\"\"\"\n    SELECT\n        id,\n        CONCAT('Record ', id) as name,\n        (id % 4) + 2020 as year,\n        CAST(RAND() * 1000 AS INT) as value\n    FROM\n        RANGE({start}, {end})\n    \"\"\")\n\n    return df\n\ndisplay(get_data_frame_with_year_column(end=10))","submissionTime":"2023-07-19T04:04:40.475GMT","completionTime":"2023-07-19T04:04:40.501GMT","stageIds":[669],"jobGroup":"64","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"83e39867-3174-4858-ae93-bbde474401f6"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 64, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"36ebc264-967f-49da-af60-e384a8eba167","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 36ebc264-967f-49da-af60-e384a8eba167)"},"metadata":{}}],"execution_count":57,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"06e5180e-5eff-4fc5-a132-ed4497bd20e0"},{"cell_type":"code","source":["def create_partitioned_table(table_path):\n","    if mssparkutils.fs.exists(table_path):\n","        mssparkutils.fs.rm(table_path, True)\n","    \n","    df = get_data_frame_with_year_column(spark)\n","    \n","    print(f\"Creating partitioned table at: {table_path}\")\n","    df.write.partitionBy(\"year\").format(\"delta\").mode(\"overwrite\").save(table_path)\n","\n","    display(df.limit(5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":65,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:04:44.6793096Z","session_start_time":null,"execution_start_time":"2023-07-19T04:04:45.1475364Z","execution_finish_time":"2023-07-19T04:04:45.5651283Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"924fa1a9-e3ba-40f5-bfa7-4558e8d2d446"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 65, Finished, Available)"},"metadata":{}}],"execution_count":58,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ec147a71-5247-4361-bd9b-606f7c7d116e"},{"cell_type":"code","source":["create_partitioned_table(table_partitioned)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":66,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:04:46.3403531Z","session_start_time":null,"execution_start_time":"2023-07-19T04:04:46.7810883Z","execution_finish_time":"2023-07-19T04:04:50.7353209Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":8,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":475,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 66:\ncreate_partitioned_table(table_partitioned)","submissionTime":"2023-07-19T04:04:49.644GMT","completionTime":"2023-07-19T04:04:49.660GMT","stageIds":[680,681],"jobGroup":"66","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":474,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 66:\ncreate_partitioned_table(table_partitioned)","submissionTime":"2023-07-19T04:04:49.607GMT","completionTime":"2023-07-19T04:04:49.631GMT","stageIds":[679],"jobGroup":"66","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":473,"name":"toString at String.java:2994","description":"Delta: Job group for statement 66:\ncreate_partitioned_table(table_partitioned): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:04:49.525GMT","completionTime":"2023-07-19T04:04:49.551GMT","stageIds":[676,677,678],"jobGroup":"66","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":472,"name":"toString at String.java:2994","description":"Delta: Job group for statement 66:\ncreate_partitioned_table(table_partitioned): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:04:49.244GMT","completionTime":"2023-07-19T04:04:49.510GMT","stageIds":[674,675],"jobGroup":"66","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":471,"name":"toString at String.java:2994","description":"Delta: Job group for statement 66:\ncreate_partitioned_table(table_partitioned): Compute snapshot for version: 0","submissionTime":"2023-07-19T04:04:49.090GMT","completionTime":"2023-07-19T04:04:49.138GMT","stageIds":[673],"jobGroup":"66","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":470,"name":"","description":"Job group for statement 66:\ncreate_partitioned_table(table_partitioned)","submissionTime":"2023-07-19T04:04:48.486GMT","completionTime":"2023-07-19T04:04:48.486GMT","stageIds":[],"jobGroup":"66","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":469,"name":"save at <unknown>:0","description":"Job group for statement 66:\ncreate_partitioned_table(table_partitioned)","submissionTime":"2023-07-19T04:04:47.242GMT","completionTime":"2023-07-19T04:04:48.406GMT","stageIds":[671,672],"jobGroup":"66","status":"SUCCEEDED","numTasks":12,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":468,"name":"save at <unknown>:0","description":"Job group for statement 66:\ncreate_partitioned_table(table_partitioned)","submissionTime":"2023-07-19T04:04:46.872GMT","completionTime":"2023-07-19T04:04:47.208GMT","stageIds":[670],"jobGroup":"66","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9a97e8c1-66d7-4df3-af2a-1ed34a5b942d"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 66, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating partitioned table at: Files/delta-lake/2-concurrency/partitioned\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"dc816ecf-9f0e-4f56-8562-b5acdf924e19","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, dc816ecf-9f0e-4f56-8562-b5acdf924e19)"},"metadata":{}}],"execution_count":59,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"1171acb4-47e8-4037-a521-e9090b5561b0"},{"cell_type":"code","source":["def merge_into_partitioned_table(task):\n","  print(f\"Starting task: {task}\")\n","  \n","  spark_clone = spark.newSession()\n","  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n","\n","  year = 2020 + task\n","\n","  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n","  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n","  dfUpdates.createOrReplaceTempView(f\"updates\")\n","\n","\n","  # Notice the year filter in the ON clause\n","  spark_clone.sql(f\"\"\"\n","      MERGE INTO delta.`{table_partitioned}` target \n","      USING updates source\n","      ON target.id = source.id AND target.year = {year}\n","      WHEN MATCHED\n","      THEN UPDATE SET target.value = source.value;\n","  \"\"\")\n","\n","  \n","  print(f\"Processed task: {task}\")\n","  return TaskResult(task, True, None)\n","\n","run_paralell_jobs(merge_into_partitioned_table, range(4))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":67,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:05:22.7694419Z","session_start_time":null,"execution_start_time":"2023-07-19T04:05:23.1947581Z","execution_finish_time":"2023-07-19T04:05:40.8987505Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":9,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":513,"name":"toString at String.java:2994","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Compute snapshot for version: 2","submissionTime":"2023-07-19T04:05:37.167GMT","completionTime":"2023-07-19T04:05:37.189GMT","stageIds":[735,736,737],"jobGroup":"67","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":512,"name":"toString at String.java:2994","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Compute snapshot for version: 2","submissionTime":"2023-07-19T04:05:36.895GMT","completionTime":"2023-07-19T04:05:37.154GMT","stageIds":[734,733],"jobGroup":"67","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":511,"name":"toString at String.java:2994","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Compute snapshot for version: 2","submissionTime":"2023-07-19T04:05:36.703GMT","completionTime":"2023-07-19T04:05:36.752GMT","stageIds":[732],"jobGroup":"67","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":505,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Writing merged data full update","submissionTime":"2023-07-19T04:05:34.513GMT","completionTime":"2023-07-19T04:05:34.862GMT","stageIds":[721,720],"jobGroup":"67","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":498,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Writing merged data full update","submissionTime":"2023-07-19T04:05:31.598GMT","completionTime":"2023-07-19T04:05:34.460GMT","stageIds":[712],"jobGroup":"67","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":495,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Writing merged no shuffle data","submissionTime":"2023-07-19T04:05:29.670GMT","completionTime":"2023-07-19T04:05:33.031GMT","stageIds":[708],"jobGroup":"67","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":488,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:05:25.375GMT","completionTime":"2023-07-19T04:05:29.409GMT","stageIds":[699,700],"jobGroup":"67","status":"SUCCEEDED","numTasks":501,"numActiveTasks":0,"numCompletedTasks":500,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":500,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":484,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:05:24.962GMT","completionTime":"2023-07-19T04:05:25.313GMT","stageIds":[694],"jobGroup":"67","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":477,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 67:\ndef merge_into_partitioned_table(task):\n  print(f\"Starting task: {task}\")\n  \n  spark_clone = spark.newSession()\n  spark_clone.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"merge into partitioned table task: {task}\")\n\n  year = 2020 + task\n\n  dfUpdates = get_data_frame_with_year_column(spark_clone, start=0, end=40_000)\n  dfUpdates = dfUpdates.where(f\"year = {year}\").withColumn(\"value\", F.expr(\"value * -1\"))\n  dfUpdates.createOrReplaceTempView(f\"updates\")\n\n\n  # Notice the year filter in the ON clause\n  spark_clone.sql(f\"\"\"\n      MERGE INTO delta.`{table_partitioned}` target \n      USING updates source\n      ON target.id = source.id AND target.year = {year}\n      WHEN MATCHED\n      THEN UPDATE SET target.value = source.value;\n  \"\"\")\n\n  \n  print(f\"Processed task: {task}\")\n  return TaskResult(task, True, None)\n\nrun_paralell_jobs(merge_into_partitioned_table, range(4)): Finding touched files - low shuffle merge","submissionTime":"2023-07-19T04:05:24.042GMT","completionTime":"2023-07-19T04:05:24.274GMT","stageIds":[684,685],"jobGroup":"67","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"43c4209c-70cb-42a3-bfbf-c20352b2ded7"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 67, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting task: 0\nStarting task: 1\nStarting task: 2\nStarting task: 3\nProcessed task: 2\nProcessed task: 3\nProcessed task: 1\nProcessed task: 0\n"]},{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"[TaskResult(task_id=0, is_successful=True, error_message=None),\n TaskResult(task_id=1, is_successful=True, error_message=None),\n TaskResult(task_id=2, is_successful=True, error_message=None),\n TaskResult(task_id=3, is_successful=True, error_message=None)]"},"metadata":{}}],"execution_count":60,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b53d4f8f-2cd4-428c-b1a9-1f2fa439d047"},{"cell_type":"code","source":["display(DeltaTable.forPath(spark, table_partitioned).history())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":68,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:05:46.7036007Z","session_start_time":null,"execution_start_time":"2023-07-19T04:05:47.1083391Z","execution_finish_time":"2023-07-19T04:05:48.1596038Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":520,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 68:\ndisplay(DeltaTable.forPath(spark, table_partitioned).history())","submissionTime":"2023-07-19T04:05:47.218GMT","completionTime":"2023-07-19T04:05:47.293GMT","stageIds":[750],"jobGroup":"68","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"200ed14d-3c57-42b8-baf0-aca65db8d518"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 68, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"5461992c-131c-47bd-b51f-0c73e23ca196","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 5461992c-131c-47bd-b51f-0c73e23ca196)"},"metadata":{}}],"execution_count":61,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"a20ddd46-375c-4e57-bb1e-b4788db5e2cd"},{"cell_type":"code","source":["display(\n","    DeltaTable.forPath(spark, table_partitioned)\n","    .toDF()\n","    .where(\"id between 39990 and 40010\")\n","    .orderBy(\"id\")\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3b7ef3e1-3a93-46d9-8394-3728ba80ed3d","statement_id":70,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-19T04:06:23.2449259Z","session_start_time":null,"execution_start_time":"2023-07-19T04:06:23.6645963Z","execution_finish_time":"2023-07-19T04:06:24.6757953Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"SUCCEEDED":2,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":526,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 70:\ndisplay(\n    DeltaTable.forPath(spark, table_partitioned)\n    .toDF()\n    .where(\"id between 39990 and 40010\")\n    .orderBy(\"id\")\n)","submissionTime":"2023-07-19T04:06:23.921GMT","completionTime":"2023-07-19T04:06:24.231GMT","stageIds":[758],"jobGroup":"70","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":525,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 70:\ndisplay(\n    DeltaTable.forPath(spark, table_partitioned)\n    .toDF()\n    .where(\"id between 39990 and 40010\")\n    .orderBy(\"id\")\n): Filtering files for query","submissionTime":"2023-07-19T04:06:23.768GMT","completionTime":"2023-07-19T04:06:23.857GMT","stageIds":[757,756],"jobGroup":"70","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7e30d73a-1c4a-44a8-94ed-493c5a59f1b3"},"text/plain":"StatementMeta(, 3b7ef3e1-3a93-46d9-8394-3728ba80ed3d, 70, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3a527061-0f05-48c8-b7f5-e74c0bd4d0ef","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3a527061-0f05-48c8-b7f5-e74c0bd4d0ef)"},"metadata":{}}],"execution_count":63,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"24529d19-abc9-44e4-b2df-b607d3d962aa"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"da7c636f-38e9-4248-85e9-f5426e6794c0"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"0835fc3f-c2ff-4ccd-9618-e51ce04890a7":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"368","index":1},{"0":"1","1":"Record 1","2":"404","index":2},{"0":"2","1":"Record 2","2":"859","index":3},{"0":"3","1":"Record 3","2":"640","index":4},{"0":"4","1":"Record 4","2":"362","index":5}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"654a07c8-87b3-42eb-8bde-db1d696e07ed":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"546","index":1},{"0":"1","1":"Record 1","2":"496","index":2},{"0":"2","1":"Record 2","2":"881","index":3},{"0":"3","1":"Record 3","2":"390","index":4},{"0":"4","1":"Record 4","2":"9","index":5}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"449233e8-5e4f-4af3-a2f0-861d72469456":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"4","1":"2023-07-19 03:52:03.877","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Append","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"true","12":{"numFiles":"8","numOutputRows":"10000","numOutputBytes":"166871"},"13":"commit from task 3","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":1},{"0":"3","1":"2023-07-19 03:52:02.8","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Append","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"true","12":{"numFiles":"8","numOutputRows":"10000","numOutputBytes":"166598"},"13":"commit from task 2","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":2},{"0":"2","1":"2023-07-19 03:52:01.583","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Append","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"true","12":{"numFiles":"8","numOutputRows":"10000","numOutputBytes":"166893"},"13":"commit from task 1","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":3},{"0":"1","1":"2023-07-19 03:52:00.364","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Append","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"true","12":{"numFiles":"8","numOutputRows":"10000","numOutputBytes":"166746"},"13":"commit from task 0","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":4},{"0":"0","1":"2023-07-19 03:51:54.402","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Overwrite","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"Serializable","11":"false","12":{"numFiles":"8","numOutputRows":"1000000","numOutputBytes":"13381001"},"13":"NULL","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":5}],"schema":[{"key":"0","name":"version","type":"bigint"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"userId","type":"string"},{"key":"3","name":"userName","type":"string"},{"key":"4","name":"operation","type":"string"},{"key":"5","name":"operationParameters","type":"MapType(StringType,StringType,true)"},{"key":"6","name":"job","type":"StructType(StructField(jobId,StringType,true),StructField(jobName,StringType,true),StructField(runId,StringType,true),StructField(jobOwnerId,StringType,true),StructField(triggerType,StringType,true))"},{"key":"7","name":"notebook","type":"StructType(StructField(notebookId,StringType,true))"},{"key":"8","name":"clusterId","type":"string"},{"key":"9","name":"readVersion","type":"bigint"},{"key":"10","name":"isolationLevel","type":"string"},{"key":"11","name":"isBlindAppend","type":"boolean"},{"key":"12","name":"operationMetrics","type":"MapType(StringType,StringType,true)"},{"key":"13","name":"userMetadata","type":"string"},{"key":"14","name":"engineInfo","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["13"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"9c1279eb-78a6-44ef-a0b3-8fe505f44855":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"255","index":1},{"0":"1","1":"Record 1","2":"100","index":2},{"0":"2","1":"Record 2","2":"348","index":3},{"0":"3","1":"Record 3","2":"141","index":4},{"0":"4","1":"Record 4","2":"60","index":5}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"7c1cad5d-db1a-40b5-a95e-0358a1fb5642":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"2023-07-19 03:53:07.109","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"(target.id = source.id)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"115000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"7435","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"3594","scanTimeMs":"3586","numTargetRowsUpdated":"10000","numOutputRows":"125000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"3378"},"13":"merge into table task: 0","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":1},{"0":"0","1":"2023-07-19 03:52:27.563","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Overwrite","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"Serializable","11":"false","12":{"numFiles":"8","numOutputRows":"1000000","numOutputBytes":"13381001"},"13":"NULL","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":2}],"schema":[{"key":"0","name":"version","type":"bigint"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"userId","type":"string"},{"key":"3","name":"userName","type":"string"},{"key":"4","name":"operation","type":"string"},{"key":"5","name":"operationParameters","type":"MapType(StringType,StringType,true)"},{"key":"6","name":"job","type":"StructType(StructField(jobId,StringType,true),StructField(jobName,StringType,true),StructField(runId,StringType,true),StructField(jobOwnerId,StringType,true),StructField(triggerType,StringType,true))"},{"key":"7","name":"notebook","type":"StructType(StructField(notebookId,StringType,true))"},{"key":"8","name":"clusterId","type":"string"},{"key":"9","name":"readVersion","type":"bigint"},{"key":"10","name":"isolationLevel","type":"string"},{"key":"11","name":"isBlindAppend","type":"boolean"},{"key":"12","name":"operationMetrics","type":"MapType(StringType,StringType,true)"},{"key":"13","name":"userMetadata","type":"string"},{"key":"14","name":"engineInfo","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["4"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"d497164b-60b1-49d3-8e64-97cd52987e13":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"763","index":1},{"0":"1","1":"Record 1","2":"289","index":2},{"0":"2","1":"Record 2","2":"21","index":3},{"0":"3","1":"Record 3","2":"490","index":4},{"0":"4","1":"Record 4","2":"621","index":5}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["2"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"2558c37f-07a7-4329-a324-003a19367ab2":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"4","1":"2023-07-19 04:04:27.568","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"(target.id = source.id)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"3","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"85000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"3303","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"631","scanTimeMs":"2479","numTargetRowsUpdated":"10000","numOutputRows":"95000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"606"},"13":"merge into table with retry task 1","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":1},{"0":"3","1":"2023-07-19 04:04:23.246","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"(target.id = source.id)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"2","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"95000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"3945","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"1204","scanTimeMs":"2309","numTargetRowsUpdated":"10000","numOutputRows":"105000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"1430"},"13":"merge into table with retry task 2","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":2},{"0":"2","1":"2023-07-19 04:04:09.293","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"(target.id = source.id)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"1","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"105000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"3824","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"810","scanTimeMs":"2765","numTargetRowsUpdated":"10000","numOutputRows":"115000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"837"},"13":"merge into table with retry task 0","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":3},{"0":"1","1":"2023-07-19 04:03:56.891","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"(target.id = source.id)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"115000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"6341","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"3255","scanTimeMs":"2683","numTargetRowsUpdated":"10000","numOutputRows":"125000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"3429"},"13":"merge into table with retry task 3","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":4},{"0":"0","1":"2023-07-19 04:03:44.314","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Overwrite","partitionBy":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"Serializable","11":"false","12":{"numFiles":"8","numOutputRows":"1000000","numOutputBytes":"13381006"},"13":"NULL","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":5}],"schema":[{"key":"0","name":"version","type":"bigint"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"userId","type":"string"},{"key":"3","name":"userName","type":"string"},{"key":"4","name":"operation","type":"string"},{"key":"5","name":"operationParameters","type":"MapType(StringType,StringType,true)"},{"key":"6","name":"job","type":"StructType(StructField(jobId,StringType,true),StructField(jobName,StringType,true),StructField(runId,StringType,true),StructField(jobOwnerId,StringType,true),StructField(triggerType,StringType,true))"},{"key":"7","name":"notebook","type":"StructType(StructField(notebookId,StringType,true))"},{"key":"8","name":"clusterId","type":"string"},{"key":"9","name":"readVersion","type":"bigint"},{"key":"10","name":"isolationLevel","type":"string"},{"key":"11","name":"isBlindAppend","type":"boolean"},{"key":"12","name":"operationMetrics","type":"MapType(StringType,StringType,true)"},{"key":"13","name":"userMetadata","type":"string"},{"key":"14","name":"engineInfo","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["4"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"36ebc264-967f-49da-af60-e384a8eba167":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"2020","3":"450","index":1},{"0":"1","1":"Record 1","2":"2021","3":"522","index":2},{"0":"2","1":"Record 2","2":"2022","3":"224","index":3},{"0":"3","1":"Record 3","2":"2023","3":"13","index":4},{"0":"4","1":"Record 4","2":"2020","3":"666","index":5},{"0":"5","1":"Record 5","2":"2021","3":"861","index":6},{"0":"6","1":"Record 6","2":"2022","3":"98","index":7},{"0":"7","1":"Record 7","2":"2023","3":"452","index":8},{"0":"8","1":"Record 8","2":"2020","3":"901","index":9},{"0":"9","1":"Record 9","2":"2021","3":"257","index":10}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"year","type":"bigint"},{"key":"3","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["3"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"dc816ecf-9f0e-4f56-8562-b5acdf924e19":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"0","1":"Record 0","2":"2020","3":"890","index":1},{"0":"1","1":"Record 1","2":"2021","3":"447","index":2},{"0":"2","1":"Record 2","2":"2022","3":"153","index":3},{"0":"3","1":"Record 3","2":"2023","3":"764","index":4},{"0":"4","1":"Record 4","2":"2020","3":"987","index":5}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"year","type":"bigint"},{"key":"3","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["3"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"5461992c-131c-47bd-b51f-0c73e23ca196":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"4","1":"2023-07-19 04:05:38.632","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"((target.id = source.id) AND (target.year = 2020L))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"240000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"11382","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"3728","scanTimeMs":"7198","numTargetRowsUpdated":"10000","numOutputRows":"250000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"3556"},"13":"merge into partitioned table task: 0","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":1},{"0":"3","1":"2023-07-19 04:05:37.484","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"((target.id = source.id) AND (target.year = 2021L))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"240000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"11158","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"1151","scanTimeMs":"9692","numTargetRowsUpdated":"10000","numOutputRows":"250000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"1052"},"13":"merge into partitioned table task: 1","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":2},{"0":"2","1":"2023-07-19 04:05:36.293","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"((target.id = source.id) AND (target.year = 2023L))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"240000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"11325","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"3613","scanTimeMs":"5599","numTargetRowsUpdated":"10000","numOutputRows":"250000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"5449"},"13":"merge into partitioned table task: 3","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":3},{"0":"1","1":"2023-07-19 04:05:34.687","2":"NULL","3":"NULL","4":"MERGE","5":{"predicate":"((target.id = source.id) AND (target.year = 2022L))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[]"},"6":"NULL","7":"NULL","8":"NULL","9":"0","10":"Serializable","11":"false","12":{"numTargetRowsCopied":"240000","numTargetRowsDeleted":"0","numTargetFilesAdded":"2","executionTimeMs":"11128","numTargetRowsInserted":"0","unmodifiedRewriteTimeMs":"6138","scanTimeMs":"3360","numTargetRowsUpdated":"10000","numOutputRows":"250000","numTargetChangeFilesAdded":"0","numSourceRows":"10000","numTargetFilesRemoved":"1","rewriteTimeMs":"7432"},"13":"merge into partitioned table task: 2","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":4},{"0":"0","1":"2023-07-19 04:04:48.68","2":"NULL","3":"NULL","4":"WRITE","5":{"mode":"Overwrite","partitionBy":"[\"year\"]"},"6":"NULL","7":"NULL","8":"NULL","9":"NULL","10":"Serializable","11":"false","12":{"numFiles":"4","numOutputRows":"1000000","numOutputBytes":"11578365"},"13":"NULL","14":"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4","index":5}],"schema":[{"key":"0","name":"version","type":"bigint"},{"key":"1","name":"timestamp","type":"timestamp"},{"key":"2","name":"userId","type":"string"},{"key":"3","name":"userName","type":"string"},{"key":"4","name":"operation","type":"string"},{"key":"5","name":"operationParameters","type":"MapType(StringType,StringType,true)"},{"key":"6","name":"job","type":"StructType(StructField(jobId,StringType,true),StructField(jobName,StringType,true),StructField(runId,StringType,true),StructField(jobOwnerId,StringType,true),StructField(triggerType,StringType,true))"},{"key":"7","name":"notebook","type":"StructType(StructField(notebookId,StringType,true))"},{"key":"8","name":"clusterId","type":"string"},{"key":"9","name":"readVersion","type":"bigint"},{"key":"10","name":"isolationLevel","type":"string"},{"key":"11","name":"isBlindAppend","type":"boolean"},{"key":"12","name":"operationMetrics","type":"MapType(StringType,StringType,true)"},{"key":"13","name":"userMetadata","type":"string"},{"key":"14","name":"engineInfo","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["4"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"3a527061-0f05-48c8-b7f5-e74c0bd4d0ef":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"39990","1":"Record 39990","2":"2022","3":"-189","index":1},{"0":"39991","1":"Record 39991","2":"2023","3":"-907","index":2},{"0":"39992","1":"Record 39992","2":"2020","3":"-931","index":3},{"0":"39993","1":"Record 39993","2":"2021","3":"-393","index":4},{"0":"39994","1":"Record 39994","2":"2022","3":"-725","index":5},{"0":"39995","1":"Record 39995","2":"2023","3":"-234","index":6},{"0":"39996","1":"Record 39996","2":"2020","3":"-236","index":7},{"0":"39997","1":"Record 39997","2":"2021","3":"0","index":8},{"0":"39998","1":"Record 39998","2":"2022","3":"-554","index":9},{"0":"39999","1":"Record 39999","2":"2023","3":"-216","index":10},{"0":"40000","1":"Record 40000","2":"2020","3":"138","index":11},{"0":"40001","1":"Record 40001","2":"2021","3":"625","index":12},{"0":"40002","1":"Record 40002","2":"2022","3":"566","index":13},{"0":"40003","1":"Record 40003","2":"2023","3":"806","index":14},{"0":"40004","1":"Record 40004","2":"2020","3":"424","index":15},{"0":"40005","1":"Record 40005","2":"2021","3":"832","index":16},{"0":"40006","1":"Record 40006","2":"2022","3":"414","index":17},{"0":"40007","1":"Record 40007","2":"2023","3":"183","index":18},{"0":"40008","1":"Record 40008","2":"2020","3":"719","index":19},{"0":"40009","1":"Record 40009","2":"2021","3":"225","index":20},{"0":"40010","1":"Record 40010","2":"2022","3":"820","index":21}],"schema":[{"key":"0","name":"id","type":"bigint"},{"key":"1","name":"name","type":"string"},{"key":"2","name":"year","type":"bigint"},{"key":"3","name":"value","type":"int"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["3"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"default_lakehouse":"ab34098a-3173-4b46-ad67-133aa4716e8b","known_lakehouses":[{"id":"ab34098a-3173-4b46-ad67-133aa4716e8b"}],"default_lakehouse_name":"MyLake","default_lakehouse_workspace_id":"339a0302-9394-4cd7-af3c-44fd076923ce"}}},"nbformat":4,"nbformat_minor":5}